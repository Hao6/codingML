{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Angle Regression(最小角回归)\n",
    "- 适用于高维度数据的线性回归\n",
    "- 可用于特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向选择（Forward Selection）算法\n",
    "- 最小角回归算法的前导算法，与最小角回归的部分原理相似\n",
    "    1. 将X看做n个m维向量，即每个属性对应一个向量；\n",
    "    2. 从n个向量中挑选与y最为接近(夹角最小，余弦最大)的向量记为$X^*$，使用y在$X^*$上的投影$\\bar{y}$来逼近y，得到残差$y = y - \\bar{y}$，其中$\\bar{y} = X^* \\times w^*$，$w^* = \\frac{X^*y}{{\\|X^*\\|}^2}$；\n",
    "    3. 判断是否已经使用了所有的n个X向量或者参差为0，若是，则退出，返回$w$.\n",
    "- 适用于较高维度的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardSelection(object):\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.train(train_x, train_y)\n",
    "\n",
    "    def train(self, train_x, train_y):\n",
    "        m , n = train_x.shape\n",
    "        train_x = np.copy(train_x)\n",
    "        if len(train_y.shape) == 1:\n",
    "            train_y = train_y.reshape(-1, 1)\n",
    "        self.theta = np.zeros((1, n))\n",
    "        # 各个向量的模\n",
    "        norm_of_vector = np.sqrt(np.sum(np.square(train_x), axis=0))\n",
    "        columns_sets = set(range(n))\n",
    "        while len(columns_sets) > 0:\n",
    "#             print train_y\n",
    "            if np.all(train_y == 0): # 残差为0\n",
    "                break\n",
    "            norm_of_y = np.sqrt(np.sum(np.square(train_y)))\n",
    "            # 计算余弦距离\n",
    "            max_cos_dis = 0\n",
    "            max_index = -1\n",
    "            for i in columns_sets:\n",
    "                cur_dis = np.sum(train_x[:, i]*train_y) / (norm_of_vector[i]*norm_of_y)\n",
    "                if abs(cur_dis) > abs(max_cos_dis):\n",
    "                    max_cos_dis = cur_dis\n",
    "                    max_index = i\n",
    "            self.theta[0][max_index] = max_cos_dis * (norm_of_y / norm_of_vector[max_index])\n",
    "            train_y -= train_x[:, max_index].reshape(-1, 1) * self.theta[0][max_index]\n",
    "            columns_sets.remove(max_index)\n",
    "            print max_index\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.theta.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向梯度（Forward Stagewise）算法\n",
    "- 最小角回归算法的前导算法，与最小角回归的部分原理相似，不像前向选择算法一样一次性使用投影，而是在最接近(余弦距离最小)的自变量$X^*$的方向上移动一小步，在观察与残差$\\bar{y}$最接近的自变量，直到残差足够小。 \n",
    "    1. 确定一个学习率$\\alpha$, 从n个向量中挑选与y最为接近(夹角最小，余弦最大)的向量记为𝑋∗，使用y在$𝑋^∗$上的投影$\\bar{y}$的一个等比例缩小$\\alpha \\bar{y}$来逼近y，得到残差$y = y - \\alpha \\bar{y}$，其中$\\bar{y} = X^* w^*$,$w^*=\\alpha \\frac{X^*y}{\\|X^*\\|}$，累加$w^*$；\n",
    "    2. 当参差非常小的时候，退出程序，返回$w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardStagewise(object):\n",
    "    def __init__(self, alpha=0.01, epsilon=0.2, epochs = 2000, verbose=False):\n",
    "        self.theta = None\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.train(train_x, train_y)\n",
    "\n",
    "    def train(self, train_x, train_y):\n",
    "        m , n = train_x.shape\n",
    "        train_x = np.copy(train_x)\n",
    "        if len(train_y.shape) == 1:\n",
    "            train_y = train_y.reshape(-1, 1)\n",
    "        self.theta = np.zeros((1, n))\n",
    "        # 各个向量的模\n",
    "        norm_of_vector = np.sqrt(np.sum(np.square(train_x), axis=0))\n",
    "        i = 0\n",
    "        while np.max(train_y) > self.epsilon and i < self.epochs:\n",
    "            norm_of_y = np.sqrt(np.sum(np.square(train_y)))\n",
    "            cur_dis = np.sum(train_x*train_y, axis=0) / (norm_of_vector*norm_of_y)\n",
    "            max_index = np.argmax(np.abs(cur_dis))\n",
    "            max_cos_dis = cur_dis[max_index]\n",
    "            \n",
    "            delta_theta = self.alpha * max_cos_dis * (norm_of_y / norm_of_vector[max_index])\n",
    "            self.theta[0][max_index] += delta_theta\n",
    "            train_y -= train_x[:, max_index].reshape(-1, 1) * delta_theta\n",
    "            i += 1\n",
    "            if self.verbose is True and i % 100 == 0:\n",
    "                print \"the %d epoch, the $\\max{y}$ %f\" \\\n",
    "                % (i, np.max(train_y))\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.theta.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小角回归\n",
    "- 前向选择算法与前向梯度算法的结合\n",
    "- 暂时未完成，待推导公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LARS(object):\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.train(train_x, train_y)\n",
    "    def train(self, train_x, train_y):\n",
    "        m , n = train_x.shape\n",
    "        train_x = np.copy(train_x)\n",
    "        if len(train_y.shape) == 1:\n",
    "            train_y = train_y.reshape(-1, 1)\n",
    "        self.theta = np.zeros((1, n))\n",
    "        # 各个向量的模\n",
    "        norm_of_vector = np.sqrt(np.sum(np.square(train_x), axis=0))\n",
    "        columns_sets = set(range(n))\n",
    "        while len(columns_sets) > 0:\n",
    "#             print train_y\n",
    "            if np.all(train_y == 0): # 残差为0\n",
    "                break\n",
    "            norm_of_y = np.sqrt(np.sum(np.square(train_y)))\n",
    "            # 计算余弦距离\n",
    "            max_cos_dis = 0\n",
    "            max_index = -1\n",
    "            for i in columns_sets:\n",
    "                cur_dis = np.sum(train_x[:, i]*train_y) / (norm_of_vector[i]*norm_of_y)\n",
    "                if abs(cur_dis) > abs(max_cos_dis):\n",
    "                    max_cos_dis = cur_dis\n",
    "                    max_index = i\n",
    "            self.theta[0][max_index] = max_cos_dis * (norm_of_y / norm_of_vector[max_index])\n",
    "            train_y -= train_x[:, max_index].reshape(-1, 1) * self.theta[0][max_index]\n",
    "            columns_sets.remove(max_index)\n",
    "            print max_index\n",
    "    def predict(self,x):\n",
    "        return np.dot(x, self.theta.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(train_x, train_y, test_x, test_y, model):\n",
    "    lr = model\n",
    "    lr.train(train_x, train_y)\n",
    "    pre_y = lr.predict(test_x)\n",
    "    \n",
    "    print('Coefficients: \\n', lr.theta)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_y, pre_y))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(test_y, pre_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_sklearn(train_x, train_y, test_x, test_y):\n",
    "    lr = linear_model.Lars()\n",
    "    lr.fit(train_x, train_y)\n",
    "    pre_y = lr.predict(test_x)\n",
    "    \n",
    "    print('Coefficients: \\n', lr.coef_, lr.intercept_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_y, pre_y))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(test_y, pre_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    # Use only one feature\n",
    "    diabetes_X = diabetes.data\n",
    "\n",
    "    # Split the data into training/testing sets\n",
    "    diabetes_X_train = diabetes_X[:-20]\n",
    "    diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "    # Split the targets into training/testing sets\n",
    "    diabetes_y_train = diabetes.target[:-20]\n",
    "    diabetes_y_test = diabetes.target[-20:]\n",
    "    return diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "6\n",
      "2\n",
      "9\n",
      "0\n",
      "3\n",
      "8\n",
      "1\n",
      "7\n",
      "('Coefficients: \\n', array([[  8636.31950834,  -3034.07551717,  11485.73831596,   6887.27470672,\n",
      "        -14440.18471778, -15512.54537759, -13395.61180117,   2049.88273377,\n",
      "          3428.58066008, -11008.72220713]]))\n",
      "Mean squared error: 4478218.34\n",
      "Variance score: -925.94\n"
     ]
    }
   ],
   "source": [
    "regression(train_x, train_y, test_x, test_y, ForwardSelection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([[  -0.39280859,    1.84270242,    0.        ,   -1.73871215,\n",
      "         150.60341141, -115.03129434,  -72.89602412,  -28.26894827,\n",
      "         -54.58247376,    3.14641988]]))\n",
      "Mean squared error: 20771.14\n",
      "Variance score: -3.30\n"
     ]
    }
   ],
   "source": [
    "regression(train_x, train_y, test_x, test_y, \n",
    "           ForwardStagewise(alpha=0.0002, epochs=500000, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([  3.23840759e-03,  -2.37443589e+02,   2.65533652e+02,\n",
      "         3.27960544e+02,  -8.54144580e+02,   5.21377321e+02,\n",
      "         1.26005906e+02,   1.96058915e+02,   6.05978036e+02,\n",
      "         7.59081972e+01]), 152.76679422403396)\n",
      "Mean squared error: 2476.04\n",
      "Variance score: 0.49\n"
     ]
    }
   ],
   "source": [
    "regression_sklearn(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
