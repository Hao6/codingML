{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "- 首先实现用于分类任务的决策树，需要对数据进行预处理：\n",
    "    1. 对label进行编码，由sklearn.LabelEncoder实现，在预测时将结果反编码\n",
    "    2. 对离散属性编码，\n",
    "    3. 对连续属性进行分段处理转化为category类型，然后编码\n",
    "    4. 使用嵌套字典形式存储树结构\n",
    "    5. 通过信息熵计算信息增益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    def get_Information_entropy(self, labels_list, n_samples):\n",
    "        \"\"\"\n",
    "        :labels_list, ndarray, 标签列表、数组\n",
    "        :n_samples, int, 总的类别数\n",
    "        计算信息熵\n",
    "        \"\"\"\n",
    "        _, label_counts = np.unique(labels_list, return_counts=True)\n",
    "        p = labels_counts*1.0 / n_samples\n",
    "        return -np.sum(p*np.log2(p))\n",
    "        \n",
    "    def __init__(self, data_x, data_y, segmentation_attr, attr_is_dispersed):\n",
    "        '''\n",
    "        :data_x: ndarray, 标签化的数据集x\n",
    "        :data_y: ndarray, 标签化的数据集y\n",
    "        :segmentation_attr, list, 当前可用的分割属性下标列表，例如[1,5,7],表示对于当前节点只有1、5、7可以\n",
    "        被用作属性分割\n",
    "        :attr_is_dispersed, ndarray, 属性是否为离散的，例如[1,0,1,0,1],则表示下标0、2、4的属性是离散的，\n",
    "        '''\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.segmentation_attr = segmentation_attr.copy()\n",
    "        self.next_nodes = {} # 存储子节点\n",
    "        \n",
    "        n_samples, n_features = data.shape\n",
    "        # 当前节点数据数不大于10、无可用分割属性、数据标签y全部一致的情况下，认定为子节点\n",
    "        uniques_y = np.unique(self.data_y, return_counts=False) \n",
    "        if n_samples <= 10 or len(self.segmentation_attr) == 0 or len(uniques_y) == 1:\n",
    "            self.is_leaf = True\n",
    "        else: # 非叶子节点\n",
    "            gain = self.get_Information_entropy(self.data_y, n_samples)\n",
    "            self.is_leaf = False\n",
    "            # 根据可用分割属性segmentation_attr，以及属性离散/连续记录表attr_is_dispersed来确定最优\n",
    "            # 分割属性,默认采用信息增益的方式，ID3算法\n",
    "            temp_gain = -1\n",
    "            temp_attr_index = -1\n",
    "            temp_attr_seg = None\n",
    "            for attr_index in self.segmentation_attr:\n",
    "                if attr_is_dispersed[attr_index] == 1: # 离散值\n",
    "                    uniques_attr, uniques_attr_counts = np.unique(\n",
    "                        self.data_x[:,attr_index], return_counts=True)\n",
    "                    temp_information_gains = []\n",
    "                    for cur_attr_label in uniques_attr:\n",
    "                        cur_mask = (self.data_x[:, self.attr_index] == cur_attr_label)\n",
    "                        temp_information_gains.append(\n",
    "                            self.get_Information_entropy(self.data_y[cur_mask], len(cur_mask)))\n",
    "                    cur_gain = np.sum((uniques_attr_counts*1.0 / n_samples) * \n",
    "                                      temp_information_gains)\n",
    "                    if temp_gain < (gain - cur_gain):\n",
    "                        temp_gain = (gain - cur_gain)\n",
    "                        temp_attr_index = attr_index\n",
    "                        temp_attr_seg = uniques_attr\n",
    "                else: # 连续值,需要寻找一个最优的二分点来分割数据，需要进行n_samples-1次尝试\n",
    "                    sort_index = np.argsort(self.data_x[:, attr_index])\n",
    "                    temp_continus_attr_seg = -1\n",
    "                    temp_continus_gain = np.inf\n",
    "                    temp_sort_index = -1\n",
    "                    for i in range(n_samples-1):\n",
    "                        temp_seg = (data_x[i, attr_index] + data_x[i+1, attr_index]) / 2.0\n",
    "                        temp_continuous_left = self.get_Information_entropy(\n",
    "                            data_y[sort_index[:i+1]], i+1)\n",
    "                        temp_continuous_right = self.get_Information_entropy(\n",
    "                            data_y[sort_index[i+1:]], n_samples-i-1)\n",
    "                        cur_continus_gains = (i+1.0)/n_samples * temp_continuous_left + \\\n",
    "                        (n_samples-i-1.0)*temp_continuous_right\n",
    "                        if temp_continus_gain > cur_continus_gains:\n",
    "                            temp_continus_gain = cur_continus_gains\n",
    "                            temp_continus_attr_seg = temp_seg\n",
    "                            temp_sort_index = i\n",
    "                        cur_gain = temp_continus_gain\n",
    "                        if temp_gain < (gain - cur_gain):\n",
    "                            temp_gain = (gain - cur_gain)\n",
    "                            temp_attr_index = attr_index\n",
    "                            temp_attr_seg = np.array([temp_sort_index])\n",
    "                            \n",
    "            # 利用最优属性进行划分，并创造该节点的子节点，保存在self.next_nodes结构中\n",
    "            \n",
    "            # 最优属性是离散值\n",
    "            self.seg_attr_index = temp_attr_index  # 该节点的分割属性（轴）的下标\n",
    "            if attr_is_dispersed[temp_attr_index] == 1:\n",
    "                self.segmentation_attr.remove(temp_attr_index) # 从备用分割属性列表中删除最优属性\n",
    "                for cur_attr_label in temp_attr_seg:\n",
    "                    cur_mask = (self.data_x[:, self.attr_index] == cur_attr_label)\n",
    "                    self.next_nodes[cur_attr_label] = TreeNode(self.data_x[cur_mask], \n",
    "                                                         self.data_y[cur_mask], \n",
    "                                                         self.segmentation_attr,\n",
    "                                                        attr_is_dispersed)\n",
    "            else: # 最优属性是连续值\n",
    "                sort_index = np.argsort(self.data_x[:, temp_attr_index])\n",
    "                seg_data = (data_x[temp_attr_index[0], temp_attr_index] + \\\n",
    "                 data_x[temp_attr_index[0]+1, temp_attr_index]) / 2.0\n",
    "                self.next_nodes[0] = TreeNode(data_x[sort_index[:temp_attr_seg[0]+1]],\n",
    "                                            data_y[sort_index[:temp_attr_seg[0]+1]],\n",
    "                                            self.segmentation_attr,\n",
    "                                            attr_is_dispersed)  # left\n",
    "                self.next_nodes[1] = TreeNode(data_x[sort_index[temp_attr_seg[0]+1:]],\n",
    "                                            data_y[sort_index[temp_attr_seg[0]+1:]],\n",
    "                                            self.segmentation_attr,\n",
    "                                            attr_is_dispersed) # right     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, train_x, train_y, attributes_classs=None, \n",
    "                 criterion='entropy'):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "        # 确定属性是连续/离散的,当属性中唯一值数量多于N/2时，认定为连续值\n",
    "        n_samples, n_features = self.train_x.shape\n",
    "        if attributes_classs is None:\n",
    "            attributes_classs = [0] * n_samples\n",
    "            for i in range(n_features):\n",
    "                uniques_i = np.unique(self.train_x[:,i])\n",
    "                if uniques_i*3 > n_samples:\n",
    "                    attributes_classs[i] = 0    \n",
    "        self.attributes_classs = attributes_classs\n",
    "        # 对离散属性、label做encoder处理\n",
    "        self.xLabelEncoders = []\n",
    "        for i in range(len(self.attributes_classs)):\n",
    "            if self.attributes_classs[i] == 0:   # 离散属性\n",
    "                cur_encoder = LabelEncoder()\n",
    "                cur_encoder.fit(self.train_x[:,i])\n",
    "                self.train_x[:,i] = cur_encoder.transform(self.train_x[:,i])\n",
    "                self.xLabelEncoders.append(cur_encoder)\n",
    "            else:\n",
    "                self.xLabelEncoders.append(None)\n",
    "        self.yLabelEncoders = LabelEncoder()\n",
    "        self.yLabelEncoders.fit(train_y)\n",
    "        train_y = self.yLabelEncoders.transform(train_y)\n",
    "        self.root = None # 根节点\n",
    "                    \n",
    "    def train(self):\n",
    "        self.root = TreeNode(train_x, train_y, range(n_features), \n",
    "                             self.attributes_classs)\n",
    "    def fit(self):\n",
    "        self.train()\n",
    "    def predict(self, test_x):\n",
    "        if self.root is None:\n",
    "            raise \"value error\"\n",
    "        n_samples, _ = test_x.shape\n",
    "        pre = np.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            cur_node = self.root\n",
    "            while cur_node.is_leaf == False:\n",
    "                cur_attr_index = cur_node.seg_attr_index\n",
    "                if self.attributes_classs[cur_attr_index] == 1: # 离散属性\n",
    "                    for key, value in cur_node.next_nodes.items():\n",
    "                        if key == test_x[i][cur_attr_index]:\n",
    "                            cur_node = value\n",
    "                else:  #连续属性\n",
    "                    if test_x[i][cur_attr_index] < cur_node.temp_attr_seg:\n",
    "                        pass\n",
    "                    else:\n",
    "                        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
