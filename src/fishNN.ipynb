{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知器\n",
    "- 与单层感知器的区别是，在形式上至少为两层，在其他方面，多层感知器使用多使用几乎处处平滑的激活函数，例如Sigmod、ReLu函数等，而单层感知器使用阶跃函数作为激活函数  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非线性激活函数 \n",
    "- 非线性激活函数给予了多层感知器“加深层数”的意义\n",
    "- 处处平滑的激活函数能够支持“通过梯度下降优化“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmod(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def h(x):\n",
    "    return (x > 0).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播的Numpy实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forward_3net(object):\n",
    "    def __init__(self):\n",
    "        self.network = {}\n",
    "        self.network['W1'] = np.array([[1,2,3], [4.5, 3.4, 7]])\n",
    "        self.network['b1'] = np.array([1,2,3])\n",
    "        self.network['W2'] = np.array([[1,2], [4.5, 3.4], [2,4]])\n",
    "        self.network['b2'] = np.array([1,2])\n",
    "        self.network['W3'] = np.array([[1,2], [4.5, 3.4]])\n",
    "        self.network['b3'] = np.array([1,2])\n",
    "    def forward(self, x):\n",
    "        A1 = np.dot(x, self.network['W1'])+self.network['b1']\n",
    "        Z1 = sigmod(A1)\n",
    "        A2 = np.dot(Z1, self.network['W2'])+self.network['b2']\n",
    "        Z2 = sigmod(A2)\n",
    "        A3 = np.dot(Z2, self.network['W3'])+self.network['b3']\n",
    "        Z3 = sigmod(A3)\n",
    "        return self.identiy_layer(Z3)\n",
    "    def identiy_layer(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99664735,  0.99843269])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_net = forward_3net()\n",
    "x = np.array([1,-9])\n",
    "f_net.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax函数，常用于输出层分类处理 \n",
    "- 将输出层的数据映射为概率，常结合经过One-Hot编码的标签数据与交叉熵函数使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return  exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00426978  0.01160646  0.03154963]\n",
      " [ 0.08576079  0.23312201  0.63369132]]\n",
      "[[ 0.73105858  0.88079708  0.95257413]\n",
      " [ 0.98201379  0.99330715  0.99752738]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "print softmax(a)\n",
    "print sigmod(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(1, x.shape[0])\n",
    "    exp_x = np.exp(x)\n",
    "    sum_x = np.sum(exp_x, axis=1)\n",
    "    return exp_x / sum_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09003057,  0.24472847,  0.66524096],\n",
       "       [ 0.09003057,  0.24472847,  0.66524096]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批处理 \n",
    "- 为了提高训练速度，常常采用批处理的方式一次加载多个训练数据，因为计算机的并行计算在矩阵计算方面可以发挥优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络（多层感知器）的学习过程 \n",
    "- 为了形式化神经网络的学习，引入损失函数等概念，形容推理能力的强弱、预测结果的好坏程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "损失函数用来评估误差大小程度\n",
    "- 均方误差\n",
    "- 交叉熵误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(x1, x2):\n",
    "    return np.sum(np.square(x1 - x2))/2\n",
    "def cross_entropy_error(t, y):\n",
    "    return -np.sum(y*np.log(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([1,5,8])\n",
    "mean_square_error(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916290731874155"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0.4,0.4,0.2])\n",
    "y = np.array([1,0,0])\n",
    "cross_entropy_error(t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini_batch版本的损失函数\n",
    "- 即每次选择一批数据作为学习的数据，然后尽可能的减小这批数据的训练误差或损失函数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t 表示预测值， y表示实际值\n",
    "def batch_cross_entropy_error(t, y):\n",
    "    if t.ndim == 1: # t 与 y 是一维数组的情况\n",
    "        t = t.reshape(1, t.shape[0])\n",
    "        y = y.reshape(1, y.shape[0])\n",
    "    return -np.sum(y*np.log(t)) / t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2628643221541276"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[0.4,0.4,0.2],[0.4,0.4,0.2]])\n",
    "y = np.array([[1,0,0],[0,0,1]])\n",
    "batch_cross_entropy_error(t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导数、偏导数、梯度\n",
    "- 训练多层感知器的数学基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值微分求近似导数\n",
    "- 前向差分、后向差分、中心差分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "# 求函数f(x)在x=x_0处的近似导数\n",
    "def numerical_diff(f, x_0):\n",
    "    h = 1e-4\n",
    "    return (f(x_0+h) - f(x_0-h)) / (2*h)\n",
    "# 求函数f(x)在x=x_0处的近似偏导数，其中x是一个一维数组\n",
    "def numerical_diff(f, x):\n",
    "    grad = np.zeros_like(x)\n",
    "    h = 1e-4\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i] += h\n",
    "        pre = f(x)\n",
    "        \n",
    "        x[i] -= 2*h\n",
    "        last = f(x)\n",
    "        \n",
    "        grad[i] = (pre - last)/(2*h)\n",
    "        x[i] +=h\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  8.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(f, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(f, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度法求最小值（局部）\n",
    "- 函数某点梯度所指的方向即是该函数在该点增长最快的方向，梯度法求极值即是一种启发式算法，沿着梯度方向前进或后退来最快增大或者减小函数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, alpha=0.01, step_num=100):\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_diff(f, init_x) # 使用数值微分求得近似导数\n",
    "        init_x -= alpha*grad\n",
    "    return init_x,f(init_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_x = np.array([3.0, 4.0])\n",
    "res_x,res = gradient_descent(f, init_x, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.11110793e-10   8.14814391e-10]\n",
      "1.0373788922e-18\n"
     ]
    }
   ],
   "source": [
    "print res_x\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  两层神经网络\n",
    "- 使用梯度下降法减小误差\n",
    "- 通过数值微分计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_layer_network(object):\n",
    "    def __init__(self):\n",
    "        self.paras = {}\n",
    "        self.paras['W1'] = np.random.randn(784,50)\n",
    "        self.paras['b1'] = np.random.randn(50)\n",
    "        self.paras['W2'] = np.random.randn(50,10)\n",
    "        self.paras['b2'] = np.random.randn(10)\n",
    "    def forward(self, x):\n",
    "        A1 = np.dot(x, self.paras['W1']) + self.paras['b1']\n",
    "        Z1 = sigmod(A1)\n",
    "        A2 = np.dot(Z1, self.paras['W2']) + self.paras['b2']\n",
    "        Z2 = batch_softmax(A2)\n",
    "        return Z2\n",
    "    def loss(self, x, y):\n",
    "        t = self.forward(x)\n",
    "#         print t.shape\n",
    "#         print y.shape\n",
    "        return batch_cross_entropy_error(t, y)\n",
    "    def accuracy(self, x, y):\n",
    "        t = self.forward(x)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        return np.sum(t == y)*1.0 / y.shape[0]\n",
    "    def numerical_diff(self, x, t):\n",
    "        h = 1e-4\n",
    "        grad = {}\n",
    "        for k,v in self.paras.items():\n",
    "            cur_para_shape = v.shape\n",
    "            grad[k] = np.zeros_like(v)\n",
    "            if len(cur_para_shape) == 1:\n",
    "                for i in range(cur_para_shape[0]):\n",
    "                    v[i] += h\n",
    "                    pre = self.loss(x, t)\n",
    "                    v[i] -= 2*h\n",
    "                    last = self.loss(x, t)\n",
    "                    grad[k][i] = (pre - last)/(2*h)\n",
    "                    v[i] +=h\n",
    "            else:\n",
    "                for i in range(cur_para_shape[0]):\n",
    "                    for j in range(cur_para_shape[1]):\n",
    "                        v[i][j] += h\n",
    "                        pre = self.loss(x, t)\n",
    "                        v[i][j] -= 2*h\n",
    "                        last = self.loss(x, t)\n",
    "                        grad[k][i][j] = (pre - last)/(2*h)\n",
    "                        v[i][j] +=h\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path = \"../data/mnist\", kind = 'train', one_hot = True, normals = True):\n",
    "    images, labels = load_mnist(path, kind=kind)\n",
    "    if normals == True:\n",
    "        images = images / 255.0\n",
    "    if one_hot == True:\n",
    "        labels_onehot = np.zeros((labels.shape[0], np.unique(labels).shape[0]))\n",
    "        for i in range(labels_onehot.shape[0]):\n",
    "            labels_onehot[i][labels[i]] = 1\n",
    "        labels = labels_onehot\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(images, labels, alpha=0.01, epochs=100, batch_size = 100, images_test=None, \n",
    "                     labels_test=None):\n",
    "    net = two_layer_network()\n",
    "    train_loss_list = []\n",
    "    train_size = images.shape[0]\n",
    "    batch_size = 100\n",
    "    for i in range(epochs):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x = images[batch_mask]\n",
    "        y = labels[batch_mask] # 抽样\n",
    "        grad = net.numerical_diff(x, y)\n",
    "#         print grad.keys()\n",
    "        for k,v in grad.items():\n",
    "            net.paras[k] -= alpha*grad[k]\n",
    "        loss = net.loss(x, y)\n",
    "        print \"has trained %d times, the train's loss %f\" % (i+1, loss)\n",
    "        if images_test is not None and labels_test is not None:\n",
    "            print net.accuracy(images_test, labels_test)\n",
    "        train_loss_list.append(loss)\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_data(kind='train')\n",
    "test_x, test_y = load_data(kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has trained 1 times, the train's loss 5.942940\n",
      "0.1113\n",
      "has trained 2 times, the train's loss 5.161636\n",
      "0.1176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0978d0b5f8ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_lost = gradient_descent(train_x, train_y, epochs = 9, alpha=0.1, images_test=test_x, \n\u001b[0;32m----> 2\u001b[0;31m                               labels_test=test_y)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-2d53a2e76e1c>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(images, labels, alpha, epochs, batch_size, images_test, labels_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 抽样\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print grad.keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-456d5aeb86ee>\u001b[0m in \u001b[0;36mnumerical_diff\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_para_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                         \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-456d5aeb86ee>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#         print t.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         print y.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-456d5aeb86ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_lost = gradient_descent(train_x, train_y, epochs = 9, alpha=0.1, images_test=test_x, \n",
    "                              labels_test=test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高效的梯度计算方法\n",
    "- 反向传播算法，虽然数值微分的方法可以求解梯度，但是运算速度太过缓慢，所以需要开发更加快速的反向传播算法\n",
    "- 链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层（layer）\n",
    "- 用层对象作为计算图的媒介来实现反向传播\n",
    "- 将层视为具有某种功能，基本的“前向传播”与“后向传播”功能，其中层对象的“属性”取决于该属性在两个基本功能中是否全局共享，如果是，则需要设置这一属性，否则不需要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加法层与乘法层 \n",
    "- 加法层对传入的梯度不做任何处理，直接传到下一层\n",
    "- 乘法层对传入的梯度进行扭转，将非本单元的输入乘以传入的梯度传出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addlayer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y): # 前向传播，输入x, y，输出 （x + y）\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, z):  # 反向传播， 输入z，表示上一层的梯度， 输出本层的梯度\n",
    "        dx = z\n",
    "        dy = z\n",
    "        return (dx, dy)\n",
    "    \n",
    "    \n",
    "class Mullayer(object):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y): # 前向传播，输入x, y，输出 （x * y）\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return self.x * self.y\n",
    "    \n",
    "    def backward(self, z):  # 反向传播， 输入z，表示上一层的梯度， 输出本层的梯度\n",
    "        dx = z * self.y\n",
    "        dy = z * self.x\n",
    "        return (dx, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单纯使用“乘法层”实现“买橘子”的前向传播与后向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the grad_apples_price is 2.200000\n",
      "the grad_apples_num is 110.000000\n",
      "the grad_sale_tax is 200.000000\n"
     ]
    }
   ],
   "source": [
    "m_lay1 = Mullayer()\n",
    "m_lay2 = Mullayer()\n",
    "apples_price = 100\n",
    "apples_num = 2\n",
    "sale_tax = 1.1\n",
    "total_prices = m_lay1.forward(apples_price, apples_num)\n",
    "total_prices_after_tax = m_lay2.forward(total_prices, sale_tax)\n",
    "grad_total_prices, grad_sale_tax = m_lay2.backward(1)\n",
    "grad_apples_price, grad_apples_num = m_lay1.backward(grad_total_prices)\n",
    "print 'the grad_apples_price is %f' % (grad_apples_price)\n",
    "print 'the grad_apples_num is %f' % (grad_apples_num)\n",
    "print 'the grad_sale_tax is %f' % (grad_sale_tax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合使用“乘法层”与“加法层”实现“买橘子与苹果”的前向传播与后向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the grad_apples_price is 2.200000\n",
      "the grad_apples_num is 110.000000\n",
      "the grad_oranges_num is 165.000000\n",
      "the grad_oranges_price is 3.300000\n",
      "the grad_sale_tax is 650.000000\n"
     ]
    }
   ],
   "source": [
    "apples_price = 100\n",
    "apples_num = 2\n",
    "sale_tax = 1.1\n",
    "oranges_price = 150\n",
    "oranges_num = 3\n",
    "\n",
    "m_lay1_1 = Mullayer()\n",
    "m_lay1_2 = Mullayer()\n",
    "a_lay2 = Addlayer()\n",
    "m_lay3 = Mullayer()\n",
    "\n",
    "# 前向传播过程\n",
    "apples_total_prices = m_lay1_1.forward(apples_num, apples_price)\n",
    "oranges_total_prices = m_lay1_2.forward(oranges_num, oranges_price)\n",
    "\n",
    "fruits_total_prices = a_lay2.forward(apples_total_prices, oranges_total_prices)\n",
    "\n",
    "fruits_total_prices_after_tax = m_lay3.forward(fruits_total_prices, sale_tax)\n",
    "\n",
    "# 后向传播过程\n",
    "df_price = 1\n",
    "grad_fruits_total_prices, grad_sale_tax = m_lay3.backward(df_price)\n",
    "\n",
    "grad_apples_total_prices, grad_oranges_total_prices = a_lay2.backward(grad_fruits_total_prices)\n",
    "\n",
    "grad_apples_num, grad_apples_price = m_lay1_1.backward(grad_apples_total_prices)\n",
    "grad_oranges_num, grad_oranges_price = m_lay1_2.backward(grad_oranges_total_prices)\n",
    "\n",
    "print 'the grad_apples_price is %f' % (grad_apples_price)\n",
    "print 'the grad_apples_num is %f' % (grad_apples_num)\n",
    "print 'the grad_oranges_num is %f' % (grad_oranges_num)\n",
    "print 'the grad_oranges_price is %f' % (grad_oranges_price)\n",
    "print 'the grad_sale_tax is %f' % (grad_sale_tax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现激活函数层\n",
    "- 将激活函数视为层，来源于计算图的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU激活函数层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_func(object):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(Activation_func):\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        x_copy = x.copy()\n",
    "        x_copy[self.mask] = 0\n",
    "        return x_copy\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation_func):\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self, x):\n",
    "        out = 1.0 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout\n",
    "        return dx * (self.out)*(1 - self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(object):\n",
    "    def __init__(self, W, b, lam=0.01):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.lam = lam\n",
    "        self.x = None\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout) + self.lam * self.W # 损失函数添加权重衰减项之后的导数 \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_cross_entropy(object):\n",
    "    def __init__(self, y):\n",
    "        self.out = None\n",
    "        self.y = y\n",
    "    def forward(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1).reshape(-1, 1))\n",
    "        sum_x = np.sum(exp_x, axis=1)\n",
    "        out = exp_x / sum_x.reshape(-1, 1)\n",
    "        self.out = out\n",
    "        return -np.sum(self.y*np.log(self.out + 1e-7)) / self.out.shape[0]\n",
    "    def backward(self, dout):\n",
    "        return dout * (self.out - self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用反向传播实现梯度下降的多层感知器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_layer_network_plus(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, is_batch_normal=False, \n",
    "                 activation_func = Sigmoid, is_weight_init = False):\n",
    "        self.layers = OrderedDict()\n",
    "        self.paras = {}\n",
    "        self.is_batch_normal = is_batch_normal\n",
    "        hidden_size.insert(0, input_size)\n",
    "        for i in range(len(hidden_size) - 1):\n",
    "            self.paras['W' + str(i)] = np.random.randn(hidden_size[i], hidden_size[i+1])\n",
    "            if is_weight_init == True:\n",
    "                if activation_func == Sigmoid:\n",
    "                    self.paras['W' + str(i)] *= (1.0/ np.sqrt(hidden_size[i]))\n",
    "                elif activation_func == ReLu:\n",
    "                    self.paras['W' + str(i)] *= (2.0 / np.sqrt(hidden_size[i]))\n",
    "            self.paras['b' + str(i)] = np.random.randn(hidden_size[i+1])\n",
    "            self.layers['Affine' + str(i)] = Affine(self.paras['W' + str(i)], self.paras['b' + str(i)])\n",
    "            if is_batch_normal == True:\n",
    "                self.layers['batch_normalization'+str(i)] = batch_normal()\n",
    "            self.layers['activation' + str(i)] = activation_func()\n",
    "            \n",
    "        self.paras['W' + str(len(hidden_size) - 1)] = np.random.randn(hidden_size[-1], output_size)\n",
    "        if is_weight_init == True:\n",
    "                if activation_func == Sigmoid:\n",
    "                    self.paras['W' + str(len(hidden_size) - 1)] *= (1.0 / np.sqrt(hidden_size[-1]))\n",
    "                elif activation_func == ReLu:\n",
    "                    self.paras['W' + str(len(hidden_size) - 1)] *= (2.0 / np.sqrt(hidden_size[-1]))\n",
    "        self.paras['b' + str(len(hidden_size) - 1)] = np.random.randn(output_size)\n",
    "        self.layers['Affine' + str(len(hidden_size) - 1)] = Affine(self.paras['W' + str(len(hidden_size) - 1)], \n",
    "                                                   self.paras['b' + str(len(hidden_size) - 1)])\n",
    "        self.lastlayer = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layers in self.layers.values():\n",
    "            x = layers.forward(x)\n",
    "        return x\n",
    "    def loss(self, x, y):\n",
    "        t = self.forward(x)\n",
    "        self.lastlayer = softmax_cross_entropy(y)\n",
    "        return self.lastlayer.forward(t)\n",
    "    def accuracy(self, x, y):\n",
    "        t = self.forward(x)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        return np.sum(t == y)*1.0 / y.shape[0]\n",
    "    def gradient(self, x, y):\n",
    "        # 前向传播\n",
    "        self.loss(x, y)\n",
    "        \n",
    "        # 后向传播\n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        \n",
    "        back_layers = self.layers.values()\n",
    "        back_layers.reverse()\n",
    "        \n",
    "        for back_layer in back_layers:\n",
    "            dout = back_layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        temp = 2\n",
    "        if self.is_batch_normal == True:\n",
    "            temp += 1  \n",
    "        for i in range(len(back_layers)/temp + 1):\n",
    "            grads['W' + str(i)] = self.layers['Affine' + str(i)].dW\n",
    "            grads['b' + str(i)] = self.layers['Affine' + str(i)].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(images, labels, alpha=0.01, spochs=100, batch_size = 100, images_test=None, labels_test=None):\n",
    "    net = two_layer_network_plus(input_size=784, hidden_size=[100, 50], output_size=10, is_batch_normal=False, \n",
    "                                 activation_func=ReLu, is_weight_init=True)\n",
    "#     print net.paras\n",
    "    train_loss_list = []\n",
    "    train_size = images.shape[0]\n",
    "    for i in range(spochs):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x = images[batch_mask]\n",
    "        y = labels[batch_mask] # 抽样\n",
    "        grad = net.gradient(x, y)\n",
    "        \n",
    "        \n",
    "        for k,v in grad.items():\n",
    "            net.paras[k] -= alpha*grad[k]\n",
    "        loss = net.loss(x, y)\n",
    "        print \"has trained %d times, the train's loss %f\" % (i+1, loss)\n",
    "        if images_test is not None and labels_test is not None:\n",
    "            print net.accuracy(images_test, labels_test)\n",
    "        train_loss_list.append(loss)\n",
    "    return net, train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has trained 1 times, the train's loss 11.409609\n",
      "0.0958\n",
      "has trained 2 times, the train's loss 7.805827\n",
      "0.1032\n",
      "has trained 3 times, the train's loss 10.429728\n",
      "0.1011\n",
      "has trained 4 times, the train's loss 13.705212\n",
      "0.098\n",
      "has trained 5 times, the train's loss 13.346472\n",
      "0.0974\n",
      "has trained 6 times, the train's loss 8.530180\n",
      "0.101\n",
      "has trained 7 times, the train's loss 7.885596\n",
      "0.0886\n",
      "has trained 8 times, the train's loss 4.588607\n",
      "0.0336\n",
      "has trained 9 times, the train's loss 2.485180\n",
      "0.0992\n",
      "has trained 10 times, the train's loss 2.423506\n",
      "0.133\n",
      "has trained 11 times, the train's loss 2.330367\n",
      "0.137\n",
      "has trained 12 times, the train's loss 2.096262\n",
      "0.2549\n",
      "has trained 13 times, the train's loss 2.208536\n",
      "0.1535\n",
      "has trained 14 times, the train's loss 2.071708\n",
      "0.2051\n",
      "has trained 15 times, the train's loss 2.965242\n",
      "0.1426\n",
      "has trained 16 times, the train's loss 2.279494\n",
      "0.1074\n",
      "has trained 17 times, the train's loss 2.330737\n",
      "0.2019\n",
      "has trained 18 times, the train's loss 2.064798\n",
      "0.2129\n",
      "has trained 19 times, the train's loss 2.005552\n",
      "0.199\n",
      "has trained 20 times, the train's loss 2.076849\n",
      "0.2083\n",
      "has trained 21 times, the train's loss 2.138129\n",
      "0.2073\n",
      "has trained 22 times, the train's loss 2.079184\n",
      "0.1943\n",
      "has trained 23 times, the train's loss 2.030880\n",
      "0.215\n",
      "has trained 24 times, the train's loss 2.091543\n",
      "0.1983\n",
      "has trained 25 times, the train's loss 2.114551\n",
      "0.2363\n",
      "has trained 26 times, the train's loss 2.033280\n",
      "0.2479\n",
      "has trained 27 times, the train's loss 1.920570\n",
      "0.2341\n",
      "has trained 28 times, the train's loss 1.946481\n",
      "0.2145\n",
      "has trained 29 times, the train's loss 2.001706\n",
      "0.2336\n",
      "has trained 30 times, the train's loss 1.833170\n",
      "0.2111\n",
      "has trained 31 times, the train's loss 1.966088\n",
      "0.2304\n",
      "has trained 32 times, the train's loss 1.907510\n",
      "0.2618\n",
      "has trained 33 times, the train's loss 1.984712\n",
      "0.2442\n",
      "has trained 34 times, the train's loss 1.904020\n",
      "0.2033\n",
      "has trained 35 times, the train's loss 1.816921\n",
      "0.2213\n",
      "has trained 36 times, the train's loss 2.024170\n",
      "0.2304\n",
      "has trained 37 times, the train's loss 1.699042\n",
      "0.1544\n",
      "has trained 38 times, the train's loss 1.890780\n",
      "0.3659\n",
      "has trained 39 times, the train's loss 1.801674\n",
      "0.3129\n",
      "has trained 40 times, the train's loss 1.792664\n",
      "0.2735\n",
      "has trained 41 times, the train's loss 2.865728\n",
      "0.2141\n",
      "has trained 42 times, the train's loss 2.163757\n",
      "0.2022\n",
      "has trained 43 times, the train's loss 2.127389\n",
      "0.211\n",
      "has trained 44 times, the train's loss 2.036086\n",
      "0.2083\n",
      "has trained 45 times, the train's loss 1.956925\n",
      "0.2102\n",
      "has trained 46 times, the train's loss 2.081225\n",
      "0.2071\n",
      "has trained 47 times, the train's loss 1.889336\n",
      "0.2014\n",
      "has trained 48 times, the train's loss 2.016779\n",
      "0.2101\n",
      "has trained 49 times, the train's loss 1.796002\n",
      "0.2096\n",
      "has trained 50 times, the train's loss 1.974175\n",
      "0.2092\n",
      "has trained 51 times, the train's loss 1.993055\n",
      "0.2192\n",
      "has trained 52 times, the train's loss 2.046661\n",
      "0.2136\n",
      "has trained 53 times, the train's loss 1.826314\n",
      "0.2592\n",
      "has trained 54 times, the train's loss 1.914826\n",
      "0.2378\n",
      "has trained 55 times, the train's loss 1.971379\n",
      "0.2341\n",
      "has trained 56 times, the train's loss 1.970077\n",
      "0.2573\n",
      "has trained 57 times, the train's loss 1.909603\n",
      "0.2766\n",
      "has trained 58 times, the train's loss 1.904570\n",
      "0.2712\n",
      "has trained 59 times, the train's loss 1.976088\n",
      "0.2311\n",
      "has trained 60 times, the train's loss 2.027909\n",
      "0.3179\n",
      "has trained 61 times, the train's loss 1.805560\n",
      "0.2873\n",
      "has trained 62 times, the train's loss 1.835348\n",
      "0.2726\n",
      "has trained 63 times, the train's loss 2.272304\n",
      "0.1617\n",
      "has trained 64 times, the train's loss 2.038655\n",
      "0.2037\n",
      "has trained 65 times, the train's loss 1.947029\n",
      "0.21\n",
      "has trained 66 times, the train's loss 1.844016\n",
      "0.2124\n",
      "has trained 67 times, the train's loss 1.966466\n",
      "0.2146\n",
      "has trained 68 times, the train's loss 1.885101\n",
      "0.2185\n",
      "has trained 69 times, the train's loss 1.835112\n",
      "0.2237\n",
      "has trained 70 times, the train's loss 1.894705\n",
      "0.2061\n",
      "has trained 71 times, the train's loss 2.002967\n",
      "0.2118\n",
      "has trained 72 times, the train's loss 1.929861\n",
      "0.213\n",
      "has trained 73 times, the train's loss 1.967927\n",
      "0.2107\n",
      "has trained 74 times, the train's loss 2.014050\n",
      "0.22\n",
      "has trained 75 times, the train's loss 1.921063\n",
      "0.2096\n",
      "has trained 76 times, the train's loss 1.889805\n",
      "0.2133\n",
      "has trained 77 times, the train's loss 1.884634\n",
      "0.2185\n",
      "has trained 78 times, the train's loss 1.978734\n",
      "0.1951\n",
      "has trained 79 times, the train's loss 2.019605\n",
      "0.214\n",
      "has trained 80 times, the train's loss 1.998846\n",
      "0.2002\n",
      "has trained 81 times, the train's loss 1.897005\n",
      "0.2179\n",
      "has trained 82 times, the train's loss 1.868829\n",
      "0.2317\n",
      "has trained 83 times, the train's loss 2.000450\n",
      "0.2246\n",
      "has trained 84 times, the train's loss 1.999202\n",
      "0.2238\n",
      "has trained 85 times, the train's loss 2.449144\n",
      "0.189\n",
      "has trained 86 times, the train's loss 2.269023\n",
      "0.0922\n",
      "has trained 87 times, the train's loss 2.086849\n",
      "0.1783\n",
      "has trained 88 times, the train's loss 2.064113\n",
      "0.2941\n",
      "has trained 89 times, the train's loss 2.330293\n",
      "0.2044\n",
      "has trained 90 times, the train's loss 2.148704\n",
      "0.191\n",
      "has trained 91 times, the train's loss 2.487969\n",
      "0.1516\n",
      "has trained 92 times, the train's loss 5.613586\n",
      "0.0618\n",
      "has trained 93 times, the train's loss 2.408410\n",
      "0.101\n",
      "has trained 94 times, the train's loss 1.881876\n",
      "0.208\n",
      "has trained 95 times, the train's loss 2.120491\n",
      "0.2084\n",
      "has trained 96 times, the train's loss 1.888916\n",
      "0.2087\n",
      "has trained 97 times, the train's loss 2.016398\n",
      "0.2123\n",
      "has trained 98 times, the train's loss 1.920846\n",
      "0.2115\n",
      "has trained 99 times, the train's loss 1.993807\n",
      "0.2111\n",
      "has trained 100 times, the train's loss 1.997842\n",
      "0.2123\n",
      "has trained 101 times, the train's loss 1.981285\n",
      "0.2144\n",
      "has trained 102 times, the train's loss 1.902065\n",
      "0.2155\n",
      "has trained 103 times, the train's loss 1.939345\n",
      "0.2148\n",
      "has trained 104 times, the train's loss 1.933635\n",
      "0.2135\n",
      "has trained 105 times, the train's loss 2.022876\n",
      "0.2073\n",
      "has trained 106 times, the train's loss 2.040654\n",
      "0.2088\n",
      "has trained 107 times, the train's loss 1.954659\n",
      "0.1977\n",
      "has trained 108 times, the train's loss 1.964594\n",
      "0.2114\n",
      "has trained 109 times, the train's loss 2.073707\n",
      "0.2113\n",
      "has trained 110 times, the train's loss 1.970050\n",
      "0.212\n",
      "has trained 111 times, the train's loss 1.924075\n",
      "0.2121\n",
      "has trained 112 times, the train's loss 2.029137\n",
      "0.2155\n",
      "has trained 113 times, the train's loss 1.923132\n",
      "0.2189\n",
      "has trained 114 times, the train's loss 1.845663\n",
      "0.2206\n",
      "has trained 115 times, the train's loss 1.958949\n",
      "0.2211\n",
      "has trained 116 times, the train's loss 1.976533\n",
      "0.2185\n",
      "has trained 117 times, the train's loss 1.880097\n",
      "0.2113\n",
      "has trained 118 times, the train's loss 2.009360\n",
      "0.2121\n",
      "has trained 119 times, the train's loss 1.841450\n",
      "0.2145\n",
      "has trained 120 times, the train's loss 1.881392\n",
      "0.2171\n",
      "has trained 121 times, the train's loss 1.981744\n",
      "0.2182\n",
      "has trained 122 times, the train's loss 2.024893\n",
      "0.2204\n",
      "has trained 123 times, the train's loss 2.007910\n",
      "0.2323\n",
      "has trained 124 times, the train's loss 1.909930\n",
      "0.2352\n",
      "has trained 125 times, the train's loss 2.034753\n",
      "0.1209\n",
      "has trained 126 times, the train's loss 1.940928\n",
      "0.2234\n",
      "has trained 127 times, the train's loss 1.844167\n",
      "0.231\n",
      "has trained 128 times, the train's loss 1.894550\n",
      "0.2653\n",
      "has trained 129 times, the train's loss 1.895890\n",
      "0.2691\n",
      "has trained 130 times, the train's loss 2.021443\n",
      "0.2186\n",
      "has trained 131 times, the train's loss 1.918570\n",
      "0.2193\n",
      "has trained 132 times, the train's loss 2.030147\n",
      "0.2217\n",
      "has trained 133 times, the train's loss 1.947065\n",
      "0.2336\n",
      "has trained 134 times, the train's loss 1.968899\n",
      "0.2266\n",
      "has trained 135 times, the train's loss 1.963314\n",
      "0.2317\n",
      "has trained 136 times, the train's loss 1.945465\n",
      "0.2329\n",
      "has trained 137 times, the train's loss 1.872936\n",
      "0.23\n",
      "has trained 138 times, the train's loss 1.878844\n",
      "0.2279\n",
      "has trained 139 times, the train's loss 1.947442\n",
      "0.234\n",
      "has trained 140 times, the train's loss 1.943713\n",
      "0.2436\n",
      "has trained 141 times, the train's loss 2.021918\n",
      "0.2399\n",
      "has trained 142 times, the train's loss 1.838162\n",
      "0.2497\n",
      "has trained 143 times, the train's loss 1.989534\n",
      "0.1987\n",
      "has trained 144 times, the train's loss 1.864130\n",
      "0.2352\n",
      "has trained 145 times, the train's loss 1.837294\n",
      "0.249\n",
      "has trained 146 times, the train's loss 2.036168\n",
      "0.234\n",
      "has trained 147 times, the train's loss 1.962266\n",
      "0.2531\n",
      "has trained 148 times, the train's loss 1.861987\n",
      "0.2572\n",
      "has trained 149 times, the train's loss 1.955366\n",
      "0.2507\n",
      "has trained 150 times, the train's loss 1.933351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2268\n",
      "has trained 151 times, the train's loss 2.324650\n",
      "0.1007\n",
      "has trained 152 times, the train's loss 1.861019\n",
      "0.207\n",
      "has trained 153 times, the train's loss 1.927164\n",
      "0.2166\n",
      "has trained 154 times, the train's loss 1.884721\n",
      "0.2203\n",
      "has trained 155 times, the train's loss 1.900129\n",
      "0.2228\n",
      "has trained 156 times, the train's loss 1.982140\n",
      "0.2263\n",
      "has trained 157 times, the train's loss 2.036490\n",
      "0.2354\n",
      "has trained 158 times, the train's loss 1.894263\n",
      "0.2329\n",
      "has trained 159 times, the train's loss 1.962553\n",
      "0.2411\n",
      "has trained 160 times, the train's loss 1.948879\n",
      "0.2332\n",
      "has trained 161 times, the train's loss 1.912547\n",
      "0.236\n",
      "has trained 162 times, the train's loss 1.991611\n",
      "0.1905\n",
      "has trained 163 times, the train's loss 1.931047\n",
      "0.2523\n",
      "has trained 164 times, the train's loss 2.051106\n",
      "0.2284\n",
      "has trained 165 times, the train's loss 1.897209\n",
      "0.2584\n",
      "has trained 166 times, the train's loss 1.913755\n",
      "0.2675\n",
      "has trained 167 times, the train's loss 2.096285\n",
      "0.1984\n",
      "has trained 168 times, the train's loss 1.953354\n",
      "0.2333\n",
      "has trained 169 times, the train's loss 2.000833\n",
      "0.1646\n",
      "has trained 170 times, the train's loss 2.132965\n",
      "0.2251\n",
      "has trained 171 times, the train's loss 2.084560\n",
      "0.2327\n",
      "has trained 172 times, the train's loss 1.960483\n",
      "0.2267\n",
      "has trained 173 times, the train's loss 1.821059\n",
      "0.2414\n",
      "has trained 174 times, the train's loss 1.880613\n",
      "0.235\n",
      "has trained 175 times, the train's loss 2.020853\n",
      "0.2385\n",
      "has trained 176 times, the train's loss 1.919184\n",
      "0.2372\n",
      "has trained 177 times, the train's loss 1.832088\n",
      "0.2679\n",
      "has trained 178 times, the train's loss 1.894566\n",
      "0.2441\n",
      "has trained 179 times, the train's loss 1.985358\n",
      "0.2062\n",
      "has trained 180 times, the train's loss 1.883099\n",
      "0.2529\n",
      "has trained 181 times, the train's loss 1.869420\n",
      "0.2942\n",
      "has trained 182 times, the train's loss 1.852347\n",
      "0.2624\n",
      "has trained 183 times, the train's loss 2.340976\n",
      "0.2488\n",
      "has trained 184 times, the train's loss 1.977826\n",
      "0.2275\n",
      "has trained 185 times, the train's loss 2.011489\n",
      "0.2458\n",
      "has trained 186 times, the train's loss 2.372136\n",
      "0.1519\n",
      "has trained 187 times, the train's loss 2.101921\n",
      "0.1872\n",
      "has trained 188 times, the train's loss 1.923877\n",
      "0.2293\n",
      "has trained 189 times, the train's loss 1.850998\n",
      "0.2281\n",
      "has trained 190 times, the train's loss 1.905296\n",
      "0.2376\n",
      "has trained 191 times, the train's loss 1.979700\n",
      "0.1395\n",
      "has trained 192 times, the train's loss 1.941853\n",
      "0.2291\n",
      "has trained 193 times, the train's loss 1.777375\n",
      "0.2561\n",
      "has trained 194 times, the train's loss 1.923265\n",
      "0.2327\n",
      "has trained 195 times, the train's loss 1.926642\n",
      "0.2642\n",
      "has trained 196 times, the train's loss 1.956284\n",
      "0.2606\n",
      "has trained 197 times, the train's loss 1.945533\n",
      "0.2403\n",
      "has trained 198 times, the train's loss 2.173436\n",
      "0.1137\n",
      "has trained 199 times, the train's loss 1.851849\n",
      "0.2297\n",
      "has trained 200 times, the train's loss 1.848018\n",
      "0.2309\n",
      "has trained 201 times, the train's loss 2.029388\n",
      "0.237\n",
      "has trained 202 times, the train's loss 1.926394\n",
      "0.2173\n",
      "has trained 203 times, the train's loss 2.052620\n",
      "0.22\n",
      "has trained 204 times, the train's loss 2.354704\n",
      "0.0967\n",
      "has trained 205 times, the train's loss 2.214265\n",
      "0.101\n",
      "has trained 206 times, the train's loss 2.057209\n",
      "0.2019\n",
      "has trained 207 times, the train's loss 1.999585\n",
      "0.233\n",
      "has trained 208 times, the train's loss 2.016751\n",
      "0.2164\n",
      "has trained 209 times, the train's loss 2.075997\n",
      "0.2211\n",
      "has trained 210 times, the train's loss 1.842583\n",
      "0.2313\n",
      "has trained 211 times, the train's loss 1.890008\n",
      "0.2278\n",
      "has trained 212 times, the train's loss 1.891695\n",
      "0.2385\n",
      "has trained 213 times, the train's loss 1.833492\n",
      "0.2504\n",
      "has trained 214 times, the train's loss 1.894254\n",
      "0.2503\n",
      "has trained 215 times, the train's loss 2.308531\n",
      "0.1133\n",
      "has trained 216 times, the train's loss 1.922341\n",
      "0.2309\n",
      "has trained 217 times, the train's loss 1.987530\n",
      "0.2461\n",
      "has trained 218 times, the train's loss 1.911126\n",
      "0.2513\n",
      "has trained 219 times, the train's loss 1.870825\n",
      "0.2331\n",
      "has trained 220 times, the train's loss 1.896781\n",
      "0.2522\n",
      "has trained 221 times, the train's loss 2.003206\n",
      "0.2439\n",
      "has trained 222 times, the train's loss 2.022568\n",
      "0.2013\n",
      "has trained 223 times, the train's loss 2.080129\n",
      "0.2208\n",
      "has trained 224 times, the train's loss 2.397422\n",
      "0.1103\n",
      "has trained 225 times, the train's loss 2.240294\n",
      "0.1226\n",
      "has trained 226 times, the train's loss 2.170506\n",
      "0.1473\n",
      "has trained 227 times, the train's loss 2.032447\n",
      "0.1532\n",
      "has trained 228 times, the train's loss 1.948882\n",
      "0.2475\n",
      "has trained 229 times, the train's loss 1.955060\n",
      "0.2539\n",
      "has trained 230 times, the train's loss 1.772785\n",
      "0.2608\n",
      "has trained 231 times, the train's loss 1.830335\n",
      "0.2645\n",
      "has trained 232 times, the train's loss 1.932109\n",
      "0.2572\n",
      "has trained 233 times, the train's loss 1.975166\n",
      "0.2551\n",
      "has trained 234 times, the train's loss 1.967136\n",
      "0.2454\n",
      "has trained 235 times, the train's loss 2.104056\n",
      "0.1569\n",
      "has trained 236 times, the train's loss 2.053653\n",
      "0.1919\n",
      "has trained 237 times, the train's loss 2.001192\n",
      "0.2335\n",
      "has trained 238 times, the train's loss 2.135464\n",
      "0.1465\n",
      "has trained 239 times, the train's loss 1.880787\n",
      "0.1743\n",
      "has trained 240 times, the train's loss 1.957847\n",
      "0.2398\n",
      "has trained 241 times, the train's loss 1.735292\n",
      "0.26\n",
      "has trained 242 times, the train's loss 1.835279\n",
      "0.2605\n",
      "has trained 243 times, the train's loss 1.938337\n",
      "0.2422\n",
      "has trained 244 times, the train's loss 2.111919\n",
      "0.1508\n",
      "has trained 245 times, the train's loss 1.965550\n",
      "0.2423\n",
      "has trained 246 times, the train's loss 2.343741\n",
      "0.1019\n",
      "has trained 247 times, the train's loss 2.273362\n",
      "0.1171\n",
      "has trained 248 times, the train's loss 2.241505\n",
      "0.1283\n",
      "has trained 249 times, the train's loss 1.880600\n",
      "0.2187\n",
      "has trained 250 times, the train's loss 1.931864\n",
      "0.2222\n",
      "has trained 251 times, the train's loss 2.334823\n",
      "0.1051\n",
      "has trained 252 times, the train's loss 2.103246\n",
      "0.1172\n",
      "has trained 253 times, the train's loss 2.026927\n",
      "0.2518\n",
      "has trained 254 times, the train's loss 1.945413\n",
      "0.2407\n",
      "has trained 255 times, the train's loss 1.974313\n",
      "0.2566\n",
      "has trained 256 times, the train's loss 1.967615\n",
      "0.2466\n",
      "has trained 257 times, the train's loss 1.923787\n",
      "0.2565\n",
      "has trained 258 times, the train's loss 1.858138\n",
      "0.247\n",
      "has trained 259 times, the train's loss 2.101137\n",
      "0.1373\n",
      "has trained 260 times, the train's loss 1.883898\n",
      "0.2579\n",
      "has trained 261 times, the train's loss 1.899830\n",
      "0.2421\n",
      "has trained 262 times, the train's loss 1.992558\n",
      "0.243\n",
      "has trained 263 times, the train's loss 2.025051\n",
      "0.1883\n",
      "has trained 264 times, the train's loss 1.962960\n",
      "0.2478\n",
      "has trained 265 times, the train's loss 2.043754\n",
      "0.2532\n",
      "has trained 266 times, the train's loss 1.907576\n",
      "0.2412\n",
      "has trained 267 times, the train's loss 1.925218\n",
      "0.251\n",
      "has trained 268 times, the train's loss 2.005625\n",
      "0.2302\n",
      "has trained 269 times, the train's loss 2.107602\n",
      "0.1467\n",
      "has trained 270 times, the train's loss 1.830718\n",
      "0.2491\n",
      "has trained 271 times, the train's loss 1.975394\n",
      "0.178\n",
      "has trained 272 times, the train's loss 1.936683\n",
      "0.2474\n",
      "has trained 273 times, the train's loss 1.773514\n",
      "0.2837\n",
      "has trained 274 times, the train's loss 1.995133\n",
      "0.2456\n",
      "has trained 275 times, the train's loss 2.309998\n",
      "0.1048\n",
      "has trained 276 times, the train's loss 2.122536\n",
      "0.1636\n",
      "has trained 277 times, the train's loss 1.865061\n",
      "0.2625\n",
      "has trained 278 times, the train's loss 1.857408\n",
      "0.2996\n",
      "has trained 279 times, the train's loss 2.060031\n",
      "0.2463\n",
      "has trained 280 times, the train's loss 2.204297\n",
      "0.214\n",
      "has trained 281 times, the train's loss 4.221225\n",
      "0.1079\n",
      "has trained 282 times, the train's loss 2.260589\n",
      "0.155\n",
      "has trained 283 times, the train's loss 2.226583\n",
      "0.1749\n",
      "has trained 284 times, the train's loss 2.076218\n",
      "0.2038\n",
      "has trained 285 times, the train's loss 2.737179\n",
      "0.2124\n",
      "has trained 286 times, the train's loss 2.419543\n",
      "0.0981\n",
      "has trained 287 times, the train's loss 2.317430\n",
      "0.1015\n",
      "has trained 288 times, the train's loss 2.201708\n",
      "0.0895\n",
      "has trained 289 times, the train's loss 2.120555\n",
      "0.1007\n",
      "has trained 290 times, the train's loss 2.004055\n",
      "0.1936\n",
      "has trained 291 times, the train's loss 2.056676\n",
      "0.2093\n",
      "has trained 292 times, the train's loss 1.940292\n",
      "0.2104\n",
      "has trained 293 times, the train's loss 2.008988\n",
      "0.2121\n",
      "has trained 294 times, the train's loss 1.974298\n",
      "0.2144\n",
      "has trained 295 times, the train's loss 1.996831\n",
      "0.2081\n",
      "has trained 296 times, the train's loss 1.970000\n",
      "0.2097\n",
      "has trained 297 times, the train's loss 1.879215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2124\n",
      "has trained 298 times, the train's loss 1.997231\n",
      "0.2064\n",
      "has trained 299 times, the train's loss 2.056826\n",
      "0.2094\n",
      "has trained 300 times, the train's loss 1.954070\n",
      "0.2071\n",
      "has trained 301 times, the train's loss 1.947313\n",
      "0.2085\n",
      "has trained 302 times, the train's loss 1.904966\n",
      "0.2128\n",
      "has trained 303 times, the train's loss 1.921168\n",
      "0.2103\n",
      "has trained 304 times, the train's loss 2.023280\n",
      "0.2142\n",
      "has trained 305 times, the train's loss 1.966427\n",
      "0.2215\n",
      "has trained 306 times, the train's loss 1.991782\n",
      "0.2204\n",
      "has trained 307 times, the train's loss 1.993466\n",
      "0.2245\n",
      "has trained 308 times, the train's loss 1.852985\n",
      "0.2233\n",
      "has trained 309 times, the train's loss 1.841233\n",
      "0.2208\n",
      "has trained 310 times, the train's loss 1.945968\n",
      "0.225\n",
      "has trained 311 times, the train's loss 2.044387\n",
      "0.2323\n",
      "has trained 312 times, the train's loss 1.934533\n",
      "0.2278\n",
      "has trained 313 times, the train's loss 1.915159\n",
      "0.2335\n",
      "has trained 314 times, the train's loss 1.998463\n",
      "0.2318\n",
      "has trained 315 times, the train's loss 1.926139\n",
      "0.2363\n",
      "has trained 316 times, the train's loss 2.002794\n",
      "0.2326\n",
      "has trained 317 times, the train's loss 1.915474\n",
      "0.2288\n",
      "has trained 318 times, the train's loss 1.850168\n",
      "0.2285\n",
      "has trained 319 times, the train's loss 1.912836\n",
      "0.2316\n",
      "has trained 320 times, the train's loss 2.023078\n",
      "0.2381\n",
      "has trained 321 times, the train's loss 1.897621\n",
      "0.2269\n",
      "has trained 322 times, the train's loss 1.934738\n",
      "0.232\n",
      "has trained 323 times, the train's loss 1.959702\n",
      "0.2215\n",
      "has trained 324 times, the train's loss 1.793676\n",
      "0.2199\n",
      "has trained 325 times, the train's loss 1.906282\n",
      "0.2444\n",
      "has trained 326 times, the train's loss 1.958010\n",
      "0.2401\n",
      "has trained 327 times, the train's loss 1.991158\n",
      "0.2407\n",
      "has trained 328 times, the train's loss 1.973034\n",
      "0.2401\n",
      "has trained 329 times, the train's loss 1.834324\n",
      "0.249\n",
      "has trained 330 times, the train's loss 2.012965\n",
      "0.2343\n",
      "has trained 331 times, the train's loss 1.807822\n",
      "0.2316\n",
      "has trained 332 times, the train's loss 1.962732\n",
      "0.2074\n",
      "has trained 333 times, the train's loss 2.328379\n",
      "0.1328\n",
      "has trained 334 times, the train's loss 1.827135\n",
      "0.2285\n",
      "has trained 335 times, the train's loss 1.887616\n",
      "0.2356\n",
      "has trained 336 times, the train's loss 2.350611\n",
      "0.1528\n",
      "has trained 337 times, the train's loss 1.919170\n",
      "0.2638\n",
      "has trained 338 times, the train's loss 1.850303\n",
      "0.2352\n",
      "has trained 339 times, the train's loss 1.960203\n",
      "0.2594\n",
      "has trained 340 times, the train's loss 1.958061\n",
      "0.2303\n",
      "has trained 341 times, the train's loss 2.292938\n",
      "0.1282\n",
      "has trained 342 times, the train's loss 1.961155\n",
      "0.2421\n",
      "has trained 343 times, the train's loss 2.072186\n",
      "0.2225\n",
      "has trained 344 times, the train's loss 2.272556\n",
      "0.1139\n",
      "has trained 345 times, the train's loss 2.236729\n",
      "0.1291\n",
      "has trained 346 times, the train's loss 2.015673\n",
      "0.1505\n",
      "has trained 347 times, the train's loss 1.891928\n",
      "0.1556\n",
      "has trained 348 times, the train's loss 1.940025\n",
      "0.2326\n",
      "has trained 349 times, the train's loss 1.906470\n",
      "0.2498\n",
      "has trained 350 times, the train's loss 1.931665\n",
      "0.2419\n",
      "has trained 351 times, the train's loss 2.000858\n",
      "0.1732\n",
      "has trained 352 times, the train's loss 2.046675\n",
      "0.2193\n",
      "has trained 353 times, the train's loss 2.219310\n",
      "0.1602\n",
      "has trained 354 times, the train's loss 1.873019\n",
      "0.1811\n",
      "has trained 355 times, the train's loss 1.924184\n",
      "0.2217\n",
      "has trained 356 times, the train's loss 2.201342\n",
      "0.1427\n",
      "has trained 357 times, the train's loss 1.781827\n",
      "0.2656\n",
      "has trained 358 times, the train's loss 1.816515\n",
      "0.2323\n",
      "has trained 359 times, the train's loss 1.884764\n",
      "0.254\n",
      "has trained 360 times, the train's loss 1.864544\n",
      "0.2285\n",
      "has trained 361 times, the train's loss 1.890831\n",
      "0.1789\n",
      "has trained 362 times, the train's loss 1.967704\n",
      "0.2667\n",
      "has trained 363 times, the train's loss 1.897993\n",
      "0.2609\n",
      "has trained 364 times, the train's loss 1.807423\n",
      "0.2403\n",
      "has trained 365 times, the train's loss 2.104337\n",
      "0.1799\n",
      "has trained 366 times, the train's loss 2.438667\n",
      "0.2135\n",
      "has trained 367 times, the train's loss 2.385461\n",
      "0.0985\n",
      "has trained 368 times, the train's loss 2.406551\n",
      "0.0983\n",
      "has trained 369 times, the train's loss 2.256224\n",
      "0.0988\n",
      "has trained 370 times, the train's loss 2.296827\n",
      "0.099\n",
      "has trained 371 times, the train's loss 2.318720\n",
      "0.0992\n",
      "has trained 372 times, the train's loss 2.288514\n",
      "0.099\n",
      "has trained 373 times, the train's loss 2.231707\n",
      "0.0989\n",
      "has trained 374 times, the train's loss 2.216699\n",
      "0.0989\n",
      "has trained 375 times, the train's loss 2.276109\n",
      "0.0982\n",
      "has trained 376 times, the train's loss 2.212097\n",
      "0.1017\n",
      "has trained 377 times, the train's loss 2.218448\n",
      "0.1017\n",
      "has trained 378 times, the train's loss 2.209349\n",
      "0.099\n",
      "has trained 379 times, the train's loss 2.189638\n",
      "0.1029\n",
      "has trained 380 times, the train's loss 2.163037\n",
      "0.0991\n",
      "has trained 381 times, the train's loss 2.208191\n",
      "0.0968\n",
      "has trained 382 times, the train's loss 2.186087\n",
      "0.1019\n",
      "has trained 383 times, the train's loss 2.165251\n",
      "0.1022\n",
      "has trained 384 times, the train's loss 2.186890\n",
      "0.1022\n",
      "has trained 385 times, the train's loss 2.185376\n",
      "0.1058\n",
      "has trained 386 times, the train's loss 2.141469\n",
      "0.2097\n",
      "has trained 387 times, the train's loss 2.172224\n",
      "0.2104\n",
      "has trained 388 times, the train's loss 2.120662\n",
      "0.2063\n",
      "has trained 389 times, the train's loss 2.165400\n",
      "0.2062\n",
      "has trained 390 times, the train's loss 2.120395\n",
      "0.2063\n",
      "has trained 391 times, the train's loss 2.070804\n",
      "0.212\n",
      "has trained 392 times, the train's loss 2.110268\n",
      "0.2069\n",
      "has trained 393 times, the train's loss 2.106903\n",
      "0.2071\n",
      "has trained 394 times, the train's loss 2.054962\n",
      "0.2071\n",
      "has trained 395 times, the train's loss 2.096258\n",
      "0.2117\n",
      "has trained 396 times, the train's loss 2.078875\n",
      "0.2113\n",
      "has trained 397 times, the train's loss 2.092164\n",
      "0.2107\n",
      "has trained 398 times, the train's loss 2.082293\n",
      "0.2119\n",
      "has trained 399 times, the train's loss 2.093765\n",
      "0.2112\n",
      "has trained 400 times, the train's loss 2.133806\n",
      "0.2092\n",
      "has trained 401 times, the train's loss 2.081593\n",
      "0.2088\n",
      "has trained 402 times, the train's loss 2.127284\n",
      "0.2113\n",
      "has trained 403 times, the train's loss 2.044137\n",
      "0.2111\n",
      "has trained 404 times, the train's loss 2.129686\n",
      "0.2113\n",
      "has trained 405 times, the train's loss 2.024225\n",
      "0.2122\n",
      "has trained 406 times, the train's loss 2.078107\n",
      "0.2089\n",
      "has trained 407 times, the train's loss 2.026612\n",
      "0.2091\n",
      "has trained 408 times, the train's loss 2.093362\n",
      "0.2106\n",
      "has trained 409 times, the train's loss 1.988801\n",
      "0.2111\n",
      "has trained 410 times, the train's loss 1.993202\n",
      "0.215\n",
      "has trained 411 times, the train's loss 2.104188\n",
      "0.2135\n",
      "has trained 412 times, the train's loss 1.992654\n",
      "0.2133\n",
      "has trained 413 times, the train's loss 2.060206\n",
      "0.2129\n",
      "has trained 414 times, the train's loss 2.021382\n",
      "0.2113\n",
      "has trained 415 times, the train's loss 2.099553\n",
      "0.2085\n",
      "has trained 416 times, the train's loss 1.977214\n",
      "0.2044\n",
      "has trained 417 times, the train's loss 2.096196\n",
      "0.2087\n",
      "has trained 418 times, the train's loss 2.053652\n",
      "0.2124\n",
      "has trained 419 times, the train's loss 1.992989\n",
      "0.2135\n",
      "has trained 420 times, the train's loss 2.109863\n",
      "0.1995\n",
      "has trained 421 times, the train's loss 2.030049\n",
      "0.2115\n",
      "has trained 422 times, the train's loss 2.088237\n",
      "0.2138\n",
      "has trained 423 times, the train's loss 2.024411\n",
      "0.2111\n",
      "has trained 424 times, the train's loss 2.055148\n",
      "0.2101\n",
      "has trained 425 times, the train's loss 2.004540\n",
      "0.2104\n",
      "has trained 426 times, the train's loss 2.085117\n",
      "0.2122\n",
      "has trained 427 times, the train's loss 2.044892\n",
      "0.2127\n",
      "has trained 428 times, the train's loss 1.944985\n",
      "0.211\n",
      "has trained 429 times, the train's loss 1.999932\n",
      "0.2114\n",
      "has trained 430 times, the train's loss 1.999452\n",
      "0.2129\n",
      "has trained 431 times, the train's loss 2.077985\n",
      "0.2125\n",
      "has trained 432 times, the train's loss 2.051618\n",
      "0.213\n",
      "has trained 433 times, the train's loss 2.020280\n",
      "0.2175\n",
      "has trained 434 times, the train's loss 2.052456\n",
      "0.2164\n",
      "has trained 435 times, the train's loss 2.048125\n",
      "0.2161\n",
      "has trained 436 times, the train's loss 1.987185\n",
      "0.213\n",
      "has trained 437 times, the train's loss 2.006469\n",
      "0.1991\n",
      "has trained 438 times, the train's loss 2.004671\n",
      "0.2096\n",
      "has trained 439 times, the train's loss 1.965290\n",
      "0.2155\n",
      "has trained 440 times, the train's loss 2.011283\n",
      "0.217\n",
      "has trained 441 times, the train's loss 1.995291\n",
      "0.2176\n",
      "has trained 442 times, the train's loss 2.002172\n",
      "0.2181\n",
      "has trained 443 times, the train's loss 2.047987\n",
      "0.1892\n",
      "has trained 444 times, the train's loss 2.084421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2145\n",
      "has trained 445 times, the train's loss 1.972479\n",
      "0.2164\n",
      "has trained 446 times, the train's loss 1.855692\n",
      "0.214\n",
      "has trained 447 times, the train's loss 1.885765\n",
      "0.2072\n",
      "has trained 448 times, the train's loss 2.062746\n",
      "0.215\n",
      "has trained 449 times, the train's loss 1.946584\n",
      "0.2153\n",
      "has trained 450 times, the train's loss 1.967490\n",
      "0.2169\n",
      "has trained 451 times, the train's loss 2.027418\n",
      "0.2163\n",
      "has trained 452 times, the train's loss 2.054673\n",
      "0.2152\n",
      "has trained 453 times, the train's loss 2.051833\n",
      "0.2176\n",
      "has trained 454 times, the train's loss 1.918889\n",
      "0.2141\n",
      "has trained 455 times, the train's loss 2.011243\n",
      "0.218\n",
      "has trained 456 times, the train's loss 1.991662\n",
      "0.2103\n",
      "has trained 457 times, the train's loss 1.959317\n",
      "0.2043\n",
      "has trained 458 times, the train's loss 2.104672\n",
      "0.2058\n",
      "has trained 459 times, the train's loss 1.985103\n",
      "0.2145\n",
      "has trained 460 times, the train's loss 1.912183\n",
      "0.2154\n",
      "has trained 461 times, the train's loss 1.990199\n",
      "0.2177\n",
      "has trained 462 times, the train's loss 2.007268\n",
      "0.2202\n",
      "has trained 463 times, the train's loss 1.785588\n",
      "0.2182\n",
      "has trained 464 times, the train's loss 2.029445\n",
      "0.2202\n",
      "has trained 465 times, the train's loss 1.973935\n",
      "0.2202\n",
      "has trained 466 times, the train's loss 1.953892\n",
      "0.2176\n",
      "has trained 467 times, the train's loss 2.047821\n",
      "0.2063\n",
      "has trained 468 times, the train's loss 1.982895\n",
      "0.2068\n",
      "has trained 469 times, the train's loss 1.944688\n",
      "0.2252\n",
      "has trained 470 times, the train's loss 1.997065\n",
      "0.225\n",
      "has trained 471 times, the train's loss 2.011637\n",
      "0.2239\n",
      "has trained 472 times, the train's loss 1.932571\n",
      "0.2198\n",
      "has trained 473 times, the train's loss 1.976028\n",
      "0.2234\n",
      "has trained 474 times, the train's loss 1.941080\n",
      "0.201\n",
      "has trained 475 times, the train's loss 1.984572\n",
      "0.2292\n",
      "has trained 476 times, the train's loss 1.902893\n",
      "0.2302\n",
      "has trained 477 times, the train's loss 1.900789\n",
      "0.2246\n",
      "has trained 478 times, the train's loss 1.859825\n",
      "0.2323\n",
      "has trained 479 times, the train's loss 1.999790\n",
      "0.2412\n",
      "has trained 480 times, the train's loss 2.013096\n",
      "0.2153\n",
      "has trained 481 times, the train's loss 2.006091\n",
      "0.1768\n",
      "has trained 482 times, the train's loss 1.978951\n",
      "0.1951\n",
      "has trained 483 times, the train's loss 1.886326\n",
      "0.2181\n",
      "has trained 484 times, the train's loss 1.880813\n",
      "0.2164\n",
      "has trained 485 times, the train's loss 2.026869\n",
      "0.2132\n",
      "has trained 486 times, the train's loss 1.852617\n",
      "0.2143\n",
      "has trained 487 times, the train's loss 2.051417\n",
      "0.2157\n",
      "has trained 488 times, the train's loss 1.995312\n",
      "0.2077\n",
      "has trained 489 times, the train's loss 1.979145\n",
      "0.2163\n",
      "has trained 490 times, the train's loss 1.912797\n",
      "0.2189\n",
      "has trained 491 times, the train's loss 1.989927\n",
      "0.22\n",
      "has trained 492 times, the train's loss 1.968902\n",
      "0.2175\n",
      "has trained 493 times, the train's loss 1.940423\n",
      "0.2168\n",
      "has trained 494 times, the train's loss 1.979656\n",
      "0.2167\n",
      "has trained 495 times, the train's loss 1.938538\n",
      "0.2186\n",
      "has trained 496 times, the train's loss 1.945320\n",
      "0.2171\n",
      "has trained 497 times, the train's loss 2.010801\n",
      "0.2185\n",
      "has trained 498 times, the train's loss 2.026606\n",
      "0.2066\n",
      "has trained 499 times, the train's loss 2.022097\n",
      "0.2195\n",
      "has trained 500 times, the train's loss 1.998503\n",
      "0.2271\n"
     ]
    }
   ],
   "source": [
    "model, train_loss = train(train_x, train_y, spochs = 500, images_test=test_x, labels_test=test_y, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与学习有关的技巧 \n",
    "- 参数的初始化\n",
    "- 超参数的选择\n",
    "- 防止过拟合的方法\n",
    "- DropOut\n",
    "- 避免陷入局部最小值\n",
    "- 最优化方法\n",
    "- batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最优化方法\n",
    "- SGD\n",
    "- Momentum\n",
    "- AdaGrad\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimize(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "class SGD(Optimize):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    def update(self, params, grads):\n",
    "        for k in params.keys():\n",
    "            params[k] -= self.alpha*grads[k]\n",
    "            \n",
    "class Momentum(Optimize):\n",
    "    def __init__(self, alpha=0.01, gamma=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.gamma*self.v[key] - self.alpha*grads[key]\n",
    "            params[key] += self.v[key]\n",
    "            \n",
    "class AdaGrad(Optimize):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.h = None\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for k in params.keys():\n",
    "                self.h[k] = np.zeros_like(params[k])\n",
    "        for k in params.keys():\n",
    "            self.h[k] += grads[k] * grads[k]\n",
    "            params[k] -= self.alpha*(grads[k] / (np.sqrt(self.h[k]) + 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用不同的最优化函数来训练神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimize(images, labels, alpha=0.01, spochs=100, batch_size = 100, images_test=None, \n",
    "                   labels_test=None, optimize=SGD()):\n",
    "    net = two_layer_network_plus(input_size=784, hidden_size=[100, 50], output_size=10)\n",
    "    train_loss_list = []\n",
    "    train_size = images.shape[0]\n",
    "    for i in range(spochs):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x = images[batch_mask]\n",
    "        y = labels[batch_mask] # 抽样\n",
    "        grad = net.gradient(x, y)\n",
    "        \n",
    "        optimize.update(net.paras, grad)\n",
    "        \n",
    "        loss = net.loss(x, y)\n",
    "        print \"has trained %d times, the train's loss %f\" % (i+1, loss)\n",
    "        if images_test is not None and labels_test is not None:\n",
    "            print net.accuracy(images_test, labels_test)\n",
    "        train_loss_list.append(loss)\n",
    "    return net, train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has trained 1 times, the train's loss 5.507310\n",
      "0.0805\n",
      "has trained 2 times, the train's loss 3.817460\n",
      "0.0731\n",
      "has trained 3 times, the train's loss 3.011187\n",
      "0.1128\n",
      "has trained 4 times, the train's loss 2.755901\n",
      "0.1417\n",
      "has trained 5 times, the train's loss 2.695483\n",
      "0.1386\n",
      "has trained 6 times, the train's loss 2.489407\n",
      "0.1735\n",
      "has trained 7 times, the train's loss 2.375706\n",
      "0.19\n",
      "has trained 8 times, the train's loss 2.287892\n",
      "0.2193\n",
      "has trained 9 times, the train's loss 2.234529\n",
      "0.1864\n",
      "has trained 10 times, the train's loss 2.346339\n",
      "0.2453\n",
      "has trained 11 times, the train's loss 2.097141\n",
      "0.2396\n",
      "has trained 12 times, the train's loss 2.155266\n",
      "0.261\n",
      "has trained 13 times, the train's loss 1.972928\n",
      "0.2853\n",
      "has trained 14 times, the train's loss 1.911358\n",
      "0.2652\n",
      "has trained 15 times, the train's loss 1.842943\n",
      "0.3217\n",
      "has trained 16 times, the train's loss 1.814077\n",
      "0.2933\n",
      "has trained 17 times, the train's loss 1.929252\n",
      "0.3023\n",
      "has trained 18 times, the train's loss 1.624896\n",
      "0.3509\n",
      "has trained 19 times, the train's loss 1.656490\n",
      "0.3816\n",
      "has trained 20 times, the train's loss 1.698083\n",
      "0.3709\n",
      "has trained 21 times, the train's loss 1.800084\n",
      "0.385\n",
      "has trained 22 times, the train's loss 1.456546\n",
      "0.4224\n",
      "has trained 23 times, the train's loss 1.504657\n",
      "0.4268\n",
      "has trained 24 times, the train's loss 1.562245\n",
      "0.4219\n",
      "has trained 25 times, the train's loss 1.454896\n",
      "0.4468\n",
      "has trained 26 times, the train's loss 1.428366\n",
      "0.4208\n",
      "has trained 27 times, the train's loss 1.554976\n",
      "0.4524\n",
      "has trained 28 times, the train's loss 1.244733\n",
      "0.4632\n",
      "has trained 29 times, the train's loss 1.317237\n",
      "0.4634\n",
      "has trained 30 times, the train's loss 1.425421\n",
      "0.4553\n",
      "has trained 31 times, the train's loss 1.286117\n",
      "0.492\n",
      "has trained 32 times, the train's loss 1.424732\n",
      "0.4367\n",
      "has trained 33 times, the train's loss 1.395356\n",
      "0.4865\n",
      "has trained 34 times, the train's loss 1.223132\n",
      "0.4736\n",
      "has trained 35 times, the train's loss 1.095328\n",
      "0.5284\n",
      "has trained 36 times, the train's loss 1.235960\n",
      "0.5538\n",
      "has trained 37 times, the train's loss 1.289997\n",
      "0.5593\n",
      "has trained 38 times, the train's loss 1.045830\n",
      "0.5596\n",
      "has trained 39 times, the train's loss 1.025102\n",
      "0.5151\n",
      "has trained 40 times, the train's loss 1.322496\n",
      "0.5697\n",
      "has trained 41 times, the train's loss 1.022925\n",
      "0.561\n",
      "has trained 42 times, the train's loss 1.188019\n",
      "0.5673\n",
      "has trained 43 times, the train's loss 0.971870\n",
      "0.5951\n",
      "has trained 44 times, the train's loss 1.088629\n",
      "0.5487\n",
      "has trained 45 times, the train's loss 0.913461\n",
      "0.5503\n",
      "has trained 46 times, the train's loss 0.972734\n",
      "0.6093\n",
      "has trained 47 times, the train's loss 1.020167\n",
      "0.5911\n",
      "has trained 48 times, the train's loss 1.083433\n",
      "0.5828\n",
      "has trained 49 times, the train's loss 1.031202\n",
      "0.5904\n",
      "has trained 50 times, the train's loss 1.110221\n",
      "0.622\n",
      "has trained 51 times, the train's loss 0.931073\n",
      "0.6363\n",
      "has trained 52 times, the train's loss 0.893021\n",
      "0.6003\n",
      "has trained 53 times, the train's loss 0.795976\n",
      "0.6299\n",
      "has trained 54 times, the train's loss 1.060024\n",
      "0.6273\n",
      "has trained 55 times, the train's loss 0.998617\n",
      "0.6532\n",
      "has trained 56 times, the train's loss 0.879196\n",
      "0.6211\n",
      "has trained 57 times, the train's loss 0.945453\n",
      "0.6381\n",
      "has trained 58 times, the train's loss 0.867299\n",
      "0.6432\n",
      "has trained 59 times, the train's loss 0.789937\n",
      "0.6535\n",
      "has trained 60 times, the train's loss 0.922270\n",
      "0.6544\n",
      "has trained 61 times, the train's loss 0.994602\n",
      "0.6364\n",
      "has trained 62 times, the train's loss 1.061813\n",
      "0.6645\n",
      "has trained 63 times, the train's loss 0.789496\n",
      "0.6597\n",
      "has trained 64 times, the train's loss 0.851116\n",
      "0.6667\n",
      "has trained 65 times, the train's loss 0.754535\n",
      "0.6713\n",
      "has trained 66 times, the train's loss 0.802323\n",
      "0.6803\n",
      "has trained 67 times, the train's loss 0.838472\n",
      "0.6568\n",
      "has trained 68 times, the train's loss 0.906607\n",
      "0.6577\n",
      "has trained 69 times, the train's loss 0.842906\n",
      "0.6843\n",
      "has trained 70 times, the train's loss 0.756278\n",
      "0.6333\n",
      "has trained 71 times, the train's loss 0.889590\n",
      "0.6776\n",
      "has trained 72 times, the train's loss 0.991782\n",
      "0.6441\n",
      "has trained 73 times, the train's loss 0.713176\n",
      "0.6649\n",
      "has trained 74 times, the train's loss 0.680820\n",
      "0.6862\n",
      "has trained 75 times, the train's loss 0.553471\n",
      "0.6983\n",
      "has trained 76 times, the train's loss 0.836294\n",
      "0.6799\n",
      "has trained 77 times, the train's loss 0.802813\n",
      "0.6909\n",
      "has trained 78 times, the train's loss 0.904742\n",
      "0.6836\n",
      "has trained 79 times, the train's loss 0.812984\n",
      "0.689\n",
      "has trained 80 times, the train's loss 0.775924\n",
      "0.702\n",
      "has trained 81 times, the train's loss 0.730034\n",
      "0.6998\n",
      "has trained 82 times, the train's loss 0.736902\n",
      "0.6992\n",
      "has trained 83 times, the train's loss 0.688680\n",
      "0.702\n",
      "has trained 84 times, the train's loss 0.788519\n",
      "0.7037\n",
      "has trained 85 times, the train's loss 0.694242\n",
      "0.6963\n",
      "has trained 86 times, the train's loss 0.751901\n",
      "0.7042\n",
      "has trained 87 times, the train's loss 0.703227\n",
      "0.7114\n",
      "has trained 88 times, the train's loss 0.724990\n",
      "0.7082\n",
      "has trained 89 times, the train's loss 0.751059\n",
      "0.7135\n",
      "has trained 90 times, the train's loss 0.659680\n",
      "0.7111\n",
      "has trained 91 times, the train's loss 0.787367\n",
      "0.7046\n",
      "has trained 92 times, the train's loss 0.801021\n",
      "0.719\n",
      "has trained 93 times, the train's loss 0.755319\n",
      "0.7173\n",
      "has trained 94 times, the train's loss 0.821265\n",
      "0.727\n",
      "has trained 95 times, the train's loss 0.722443\n",
      "0.7222\n",
      "has trained 96 times, the train's loss 0.756584\n",
      "0.7246\n",
      "has trained 97 times, the train's loss 0.720070\n",
      "0.7186\n",
      "has trained 98 times, the train's loss 0.673590\n",
      "0.7206\n",
      "has trained 99 times, the train's loss 0.769050\n",
      "0.7162\n",
      "has trained 100 times, the train's loss 0.598860\n",
      "0.7331\n"
     ]
    }
   ],
   "source": [
    "model, train_loss = train_optimize(train_x, train_y, spochs = 100, images_test=test_x, \n",
    "                                   labels_test=test_y, alpha=0.01, optimize = SGD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has trained 1 times, the train's loss 5.937040\n",
      "0.0764\n",
      "has trained 2 times, the train's loss 6.624808\n",
      "0.1139\n",
      "has trained 3 times, the train's loss 5.934610\n",
      "0.1582\n",
      "has trained 4 times, the train's loss 7.558253\n",
      "0.1717\n",
      "has trained 5 times, the train's loss 7.181464\n",
      "0.1609\n",
      "has trained 6 times, the train's loss 6.768113\n",
      "0.1474\n",
      "has trained 7 times, the train's loss 4.825473\n",
      "0.1267\n",
      "has trained 8 times, the train's loss 4.816430\n",
      "0.1759\n",
      "has trained 9 times, the train's loss 4.781615\n",
      "0.1975\n",
      "has trained 10 times, the train's loss 3.632024\n",
      "0.1827\n",
      "has trained 11 times, the train's loss 2.495004\n",
      "0.1929\n",
      "has trained 12 times, the train's loss 3.138211\n",
      "0.1654\n",
      "has trained 13 times, the train's loss 2.648132\n",
      "0.2156\n",
      "has trained 14 times, the train's loss 3.065370\n",
      "0.185\n",
      "has trained 15 times, the train's loss 2.699620\n",
      "0.2816\n",
      "has trained 16 times, the train's loss 2.341536\n",
      "0.2599\n",
      "has trained 17 times, the train's loss 2.160864\n",
      "0.2843\n",
      "has trained 18 times, the train's loss 2.353396\n",
      "0.2125\n",
      "has trained 19 times, the train's loss 2.627357\n",
      "0.194\n",
      "has trained 20 times, the train's loss 2.191115\n",
      "0.2157\n",
      "has trained 21 times, the train's loss 1.879415\n",
      "0.2616\n",
      "has trained 22 times, the train's loss 2.350074\n",
      "0.3429\n",
      "has trained 23 times, the train's loss 2.008250\n",
      "0.3654\n",
      "has trained 24 times, the train's loss 1.663990\n",
      "0.3709\n",
      "has trained 25 times, the train's loss 1.635865\n",
      "0.3456\n",
      "has trained 26 times, the train's loss 1.805340\n",
      "0.3437\n",
      "has trained 27 times, the train's loss 1.724108\n",
      "0.3688\n",
      "has trained 28 times, the train's loss 1.317149\n",
      "0.4459\n",
      "has trained 29 times, the train's loss 1.441460\n",
      "0.496\n",
      "has trained 30 times, the train's loss 1.347194\n",
      "0.4772\n",
      "has trained 31 times, the train's loss 1.197057\n",
      "0.4872\n",
      "has trained 32 times, the train's loss 1.089252\n",
      "0.5549\n",
      "has trained 33 times, the train's loss 1.345767\n",
      "0.5222\n",
      "has trained 34 times, the train's loss 1.092656\n",
      "0.4602\n",
      "has trained 35 times, the train's loss 1.383372\n",
      "0.484\n",
      "has trained 36 times, the train's loss 1.262470\n",
      "0.5105\n",
      "has trained 37 times, the train's loss 1.105245\n",
      "0.5697\n",
      "has trained 38 times, the train's loss 1.175939\n",
      "0.5619\n",
      "has trained 39 times, the train's loss 1.094932\n",
      "0.554\n",
      "has trained 40 times, the train's loss 1.265675\n",
      "0.5501\n",
      "has trained 41 times, the train's loss 1.246777\n",
      "0.6029\n",
      "has trained 42 times, the train's loss 1.077173\n",
      "0.6084\n",
      "has trained 43 times, the train's loss 1.055928\n",
      "0.5855\n",
      "has trained 44 times, the train's loss 1.177461\n",
      "0.5572\n",
      "has trained 45 times, the train's loss 1.137793\n",
      "0.4932\n",
      "has trained 46 times, the train's loss 1.172433\n",
      "0.5153\n",
      "has trained 47 times, the train's loss 0.805320\n",
      "0.6313\n",
      "has trained 48 times, the train's loss 0.943316\n",
      "0.6497\n",
      "has trained 49 times, the train's loss 1.030443\n",
      "0.6372\n",
      "has trained 50 times, the train's loss 0.845023\n",
      "0.6538\n",
      "has trained 51 times, the train's loss 0.951001\n",
      "0.6537\n",
      "has trained 52 times, the train's loss 0.853035\n",
      "0.6445\n",
      "has trained 53 times, the train's loss 1.112173\n",
      "0.6556\n",
      "has trained 54 times, the train's loss 1.108740\n",
      "0.6273\n",
      "has trained 55 times, the train's loss 0.833067\n",
      "0.6668\n",
      "has trained 56 times, the train's loss 0.871858\n",
      "0.6526\n",
      "has trained 57 times, the train's loss 0.890546\n",
      "0.6603\n",
      "has trained 58 times, the train's loss 1.170447\n",
      "0.7041\n",
      "has trained 59 times, the train's loss 0.836624\n",
      "0.7009\n",
      "has trained 60 times, the train's loss 0.931980\n",
      "0.6809\n",
      "has trained 61 times, the train's loss 0.702783\n",
      "0.6901\n",
      "has trained 62 times, the train's loss 0.766273\n",
      "0.711\n",
      "has trained 63 times, the train's loss 0.653593\n",
      "0.7003\n",
      "has trained 64 times, the train's loss 0.810637\n",
      "0.6637\n",
      "has trained 65 times, the train's loss 0.940547\n",
      "0.6851\n",
      "has trained 66 times, the train's loss 1.025139\n",
      "0.696\n",
      "has trained 67 times, the train's loss 0.681304\n",
      "0.745\n",
      "has trained 68 times, the train's loss 1.034845\n",
      "0.7058\n",
      "has trained 69 times, the train's loss 0.994028\n",
      "0.6858\n",
      "has trained 70 times, the train's loss 0.761418\n",
      "0.7114\n",
      "has trained 71 times, the train's loss 0.496595\n",
      "0.7488\n",
      "has trained 72 times, the train's loss 0.958328\n",
      "0.7171\n",
      "has trained 73 times, the train's loss 0.816278\n",
      "0.6709\n",
      "has trained 74 times, the train's loss 0.750459\n",
      "0.679\n",
      "has trained 75 times, the train's loss 0.807334\n",
      "0.681\n",
      "has trained 76 times, the train's loss 0.838114\n",
      "0.7034\n",
      "has trained 77 times, the train's loss 0.700377\n",
      "0.7368\n",
      "has trained 78 times, the train's loss 0.712748\n",
      "0.7352\n",
      "has trained 79 times, the train's loss 0.691145\n",
      "0.7652\n",
      "has trained 80 times, the train's loss 0.568877\n",
      "0.7784\n",
      "has trained 81 times, the train's loss 0.546893\n",
      "0.7608\n",
      "has trained 82 times, the train's loss 0.670433\n",
      "0.747\n",
      "has trained 83 times, the train's loss 0.730122\n",
      "0.7359\n",
      "has trained 84 times, the train's loss 0.667148\n",
      "0.7551\n",
      "has trained 85 times, the train's loss 0.324775\n",
      "0.8043\n",
      "has trained 86 times, the train's loss 0.534065\n",
      "0.7921\n",
      "has trained 87 times, the train's loss 0.786619\n",
      "0.7742\n",
      "has trained 88 times, the train's loss 0.528209\n",
      "0.7777\n",
      "has trained 89 times, the train's loss 0.372021\n",
      "0.804\n",
      "has trained 90 times, the train's loss 0.373113\n",
      "0.81\n",
      "has trained 91 times, the train's loss 0.697084\n",
      "0.7736\n",
      "has trained 92 times, the train's loss 0.616237\n",
      "0.7742\n",
      "has trained 93 times, the train's loss 0.736248\n",
      "0.7995\n",
      "has trained 94 times, the train's loss 0.576175\n",
      "0.7959\n",
      "has trained 95 times, the train's loss 0.487100\n",
      "0.7775\n",
      "has trained 96 times, the train's loss 0.510804\n",
      "0.784\n",
      "has trained 97 times, the train's loss 0.570757\n",
      "0.7837\n",
      "has trained 98 times, the train's loss 0.784634\n",
      "0.7703\n",
      "has trained 99 times, the train's loss 0.563880\n",
      "0.767\n",
      "has trained 100 times, the train's loss 0.642694\n",
      "0.782\n"
     ]
    }
   ],
   "source": [
    "model, train_loss = train_optimize(train_x, train_y, spochs = 100, images_test=test_x, \n",
    "                                   labels_test=test_y, alpha=0.01, optimize = Momentum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has trained 1 times, the train's loss 6.165890\n",
      "0.1206\n",
      "has trained 2 times, the train's loss 4.961351\n",
      "0.1251\n",
      "has trained 3 times, the train's loss 5.011320\n",
      "0.1211\n",
      "has trained 4 times, the train's loss 4.649020\n",
      "0.1216\n",
      "has trained 5 times, the train's loss 3.923908\n",
      "0.126\n",
      "has trained 6 times, the train's loss 3.953746\n",
      "0.1336\n",
      "has trained 7 times, the train's loss 3.487673\n",
      "0.1407\n",
      "has trained 8 times, the train's loss 3.821155\n",
      "0.1464\n",
      "has trained 9 times, the train's loss 3.437173\n",
      "0.1525\n",
      "has trained 10 times, the train's loss 3.286322\n",
      "0.1677\n",
      "has trained 11 times, the train's loss 3.858791\n",
      "0.1734\n",
      "has trained 12 times, the train's loss 3.333931\n",
      "0.1811\n",
      "has trained 13 times, the train's loss 2.993027\n",
      "0.1887\n",
      "has trained 14 times, the train's loss 3.010816\n",
      "0.1966\n",
      "has trained 15 times, the train's loss 2.976419\n",
      "0.2048\n",
      "has trained 16 times, the train's loss 2.893185\n",
      "0.2064\n",
      "has trained 17 times, the train's loss 2.960886\n",
      "0.2174\n",
      "has trained 18 times, the train's loss 3.016327\n",
      "0.2253\n",
      "has trained 19 times, the train's loss 2.930876\n",
      "0.2297\n",
      "has trained 20 times, the train's loss 2.418699\n",
      "0.238\n",
      "has trained 21 times, the train's loss 2.528162\n",
      "0.2449\n",
      "has trained 22 times, the train's loss 2.352539\n",
      "0.2553\n",
      "has trained 23 times, the train's loss 2.501233\n",
      "0.2633\n",
      "has trained 24 times, the train's loss 2.367129\n",
      "0.2681\n",
      "has trained 25 times, the train's loss 1.974946\n",
      "0.276\n",
      "has trained 26 times, the train's loss 2.277923\n",
      "0.2818\n",
      "has trained 27 times, the train's loss 2.176123\n",
      "0.2854\n",
      "has trained 28 times, the train's loss 2.134866\n",
      "0.2903\n",
      "has trained 29 times, the train's loss 2.051369\n",
      "0.2982\n",
      "has trained 30 times, the train's loss 2.172675\n",
      "0.3048\n",
      "has trained 31 times, the train's loss 2.029129\n",
      "0.3114\n",
      "has trained 32 times, the train's loss 2.256959\n",
      "0.3166\n",
      "has trained 33 times, the train's loss 2.259391\n",
      "0.3207\n",
      "has trained 34 times, the train's loss 1.957640\n",
      "0.3293\n",
      "has trained 35 times, the train's loss 1.959194\n",
      "0.3343\n",
      "has trained 36 times, the train's loss 2.108474\n",
      "0.3417\n",
      "has trained 37 times, the train's loss 2.050831\n",
      "0.3462\n",
      "has trained 38 times, the train's loss 1.752932\n",
      "0.3522\n",
      "has trained 39 times, the train's loss 1.872004\n",
      "0.3561\n",
      "has trained 40 times, the train's loss 1.845388\n",
      "0.3625\n",
      "has trained 41 times, the train's loss 1.695521\n",
      "0.3662\n",
      "has trained 42 times, the train's loss 2.042427\n",
      "0.3693\n",
      "has trained 43 times, the train's loss 1.759640\n",
      "0.373\n",
      "has trained 44 times, the train's loss 2.097699\n",
      "0.3763\n",
      "has trained 45 times, the train's loss 1.842641\n",
      "0.3803\n",
      "has trained 46 times, the train's loss 1.998388\n",
      "0.386\n",
      "has trained 47 times, the train's loss 1.627561\n",
      "0.3896\n",
      "has trained 48 times, the train's loss 1.967115\n",
      "0.3975\n",
      "has trained 49 times, the train's loss 1.723867\n",
      "0.4013\n",
      "has trained 50 times, the train's loss 1.862566\n",
      "0.4051\n",
      "has trained 51 times, the train's loss 1.924681\n",
      "0.4064\n",
      "has trained 52 times, the train's loss 1.714626\n",
      "0.4091\n",
      "has trained 53 times, the train's loss 1.945300\n",
      "0.413\n",
      "has trained 54 times, the train's loss 1.873848\n",
      "0.4146\n",
      "has trained 55 times, the train's loss 1.749821\n",
      "0.4192\n",
      "has trained 56 times, the train's loss 1.720152\n",
      "0.4232\n",
      "has trained 57 times, the train's loss 1.850504\n",
      "0.426\n",
      "has trained 58 times, the train's loss 1.710704\n",
      "0.4289\n",
      "has trained 59 times, the train's loss 1.469601\n",
      "0.4315\n",
      "has trained 60 times, the train's loss 1.553367\n",
      "0.4324\n",
      "has trained 61 times, the train's loss 1.668241\n",
      "0.4359\n",
      "has trained 62 times, the train's loss 1.570523\n",
      "0.4367\n",
      "has trained 63 times, the train's loss 1.668283\n",
      "0.4412\n",
      "has trained 64 times, the train's loss 1.402012\n",
      "0.4449\n",
      "has trained 65 times, the train's loss 1.672888\n",
      "0.4498\n",
      "has trained 66 times, the train's loss 1.798767\n",
      "0.451\n",
      "has trained 67 times, the train's loss 1.603333\n",
      "0.4534\n",
      "has trained 68 times, the train's loss 1.622195\n",
      "0.4547\n",
      "has trained 69 times, the train's loss 1.741098\n",
      "0.4542\n",
      "has trained 70 times, the train's loss 1.646769\n",
      "0.4597\n",
      "has trained 71 times, the train's loss 1.674199\n",
      "0.4596\n",
      "has trained 72 times, the train's loss 1.710956\n",
      "0.4623\n",
      "has trained 73 times, the train's loss 1.632018\n",
      "0.465\n",
      "has trained 74 times, the train's loss 1.701532\n",
      "0.4684\n",
      "has trained 75 times, the train's loss 1.566992\n",
      "0.4694\n",
      "has trained 76 times, the train's loss 1.626140\n",
      "0.4711\n",
      "has trained 77 times, the train's loss 1.522406\n",
      "0.4738\n",
      "has trained 78 times, the train's loss 1.792230\n",
      "0.4772\n",
      "has trained 79 times, the train's loss 1.365415\n",
      "0.4783\n",
      "has trained 80 times, the train's loss 1.348558\n",
      "0.4791\n",
      "has trained 81 times, the train's loss 1.694285\n",
      "0.4802\n",
      "has trained 82 times, the train's loss 1.618023\n",
      "0.4828\n",
      "has trained 83 times, the train's loss 1.617807\n",
      "0.4871\n",
      "has trained 84 times, the train's loss 1.770150\n",
      "0.4898\n",
      "has trained 85 times, the train's loss 1.539973\n",
      "0.4905\n",
      "has trained 86 times, the train's loss 1.633440\n",
      "0.4905\n",
      "has trained 87 times, the train's loss 1.468868\n",
      "0.4928\n",
      "has trained 88 times, the train's loss 1.656920\n",
      "0.4968\n",
      "has trained 89 times, the train's loss 1.503751\n",
      "0.4976\n",
      "has trained 90 times, the train's loss 1.302890\n",
      "0.4987\n",
      "has trained 91 times, the train's loss 1.378781\n",
      "0.4987\n",
      "has trained 92 times, the train's loss 1.504620\n",
      "0.5004\n",
      "has trained 93 times, the train's loss 1.642622\n",
      "0.503\n",
      "has trained 94 times, the train's loss 1.636824\n",
      "0.5047\n",
      "has trained 95 times, the train's loss 1.355408\n",
      "0.5061\n",
      "has trained 96 times, the train's loss 1.320698\n",
      "0.5083\n",
      "has trained 97 times, the train's loss 1.397337\n",
      "0.5116\n",
      "has trained 98 times, the train's loss 1.590551\n",
      "0.5133\n",
      "has trained 99 times, the train's loss 1.387571\n",
      "0.5167\n",
      "has trained 100 times, the train's loss 1.494335\n",
      "0.5171\n"
     ]
    }
   ],
   "source": [
    "model, train_loss = train_optimize(train_x, train_y, spochs = 100, images_test=test_x, \n",
    "                                   labels_test=test_y, alpha=0.01, optimize = AdaGrad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重的初始值设置方法\n",
    "- 可以全部初始化为0吗？答曰不能，会导致该层所有的权重在训练过程中保持不变，由“乘法层”的实现得知\n",
    "- 那该如何设置呢？有答案，Xavier初始值,适合Sigmod、tanh激活函数，He初始值，适合ReLU激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐藏层权重的分布实验 \n",
    "- 正太分布生成1000条100维的数据，5层的多层感知器，每层100个节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(sd=1, activation_func='Sigmoid'):\n",
    "    x = np.random.randn(1000, 100)\n",
    "    node_num = 100\n",
    "    hidden_size = 5\n",
    "    activations = {} \n",
    "\n",
    "    for i in range(hidden_size):\n",
    "        w = np.random.randn(node_num, node_num) * sd\n",
    "        a = np.dot(x, w)\n",
    "        if activation_func == 'ReLU':\n",
    "            x = reLu(a)\n",
    "        else:\n",
    "            x = sigmod(a)\n",
    "        activations[i] = x\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(activations):\n",
    "    for k,v in activations.items():\n",
    "        plt.subplot(1, len(activations), k+1)\n",
    "        plt.title(str(k+1) + 'layer')\n",
    "        plt.hist(v.flatten(), 30, range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hist(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations1 = experiment(sd=0.01)\n",
    "plot_hist(activations1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验结论\n",
    "- 根据上述的实验，可以看出，如果使用符合一般正态分布的随机值初始化权重，将引起权重分布机器不均匀，而最终导致“梯度消失”，图1，或者“表现力受限”，图2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier初始值 \n",
    "- 当激活函数为Sigmoid、tanh等中间部分可看做线性的激活函数时，Xavier初始值为sqrt(1/n),其中n为前一层神经元个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations_xavier = experiment(sd = 0.1)\n",
    "plot_hist(activations_xavier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He初始值 \n",
    "- 当激活函数为ReLU函数时，He初始值为sqrt(2/n),其中n为前一层神经元个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations2 = experiment(sd=np.sqrt(0.02), activation_func='RelU')\n",
    "plot_hist(activations2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations3 = experiment(sd=0.6, activation_func='RelU')\n",
    "plot_hist(activations3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验结论\n",
    "- Xavier、He初始值明显改善了权重的分布，更加“均匀”，尽量避免了“梯度消失”或者“表现力受限”的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization （局部归一化）\n",
    "- 加速学习过程\n",
    "- 降低对初始权重值的依赖\n",
    "- 抑制过拟合\n",
    "- 调整各层激活值的分布，使其拥有适当广度，通过Batch Normalization层来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_forward(x, gamma, beta, eps):\n",
    "    N, D = x.shape\n",
    "    \n",
    "    # 计算每一维度的平均值\n",
    "    x_mean = np.mean(x, axis=0) # (D,)\n",
    "    \n",
    "    # 计算每一维度的方差\n",
    "    diff = x - x_mean #(N, D)\n",
    "    diff_square = np.square(diff)\n",
    "    mean_diff_square = np.mean(diff_square, axis=0) #(D,)\n",
    "    \n",
    "    # 计算归一化之后的x\n",
    "    sqrt_mean_diff_square = np.sqrt(mean_diff_square + eps)  # (D, )\n",
    "    x_ = diff / sqrt_mean_diff_square # （N, D)\n",
    "    \n",
    "    # 输出\n",
    "    y = gamma * x_ + beta # (N, D)\n",
    "    \n",
    "    return y,(x_mean, sigma_square, x_)\n",
    "\n",
    "def bn_backward(dout, x_):\n",
    "    dBeta = dout\n",
    "    dGamma = dout * x_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_normal(object):\n",
    "    def __init__(self, gamma = 1, beta = 0, eps=10e-7):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        self.mu = None\n",
    "        self.xmu = None\n",
    "        self.sq = None\n",
    "        self.var = None\n",
    "        self.sqrtvar = None\n",
    "        self.ivar = None\n",
    "        self.xhat = None\n",
    "        self.gammax = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, D = x.shape\n",
    "    \n",
    "        # 计算每一维度的平均值\n",
    "        self.mu = np.mean(x, axis=0) # (D,)\n",
    "\n",
    "        # 计算每一维度的方差\n",
    "        self.xmu = x - self.mu #(N, D)\n",
    "        self.sq = np.square(self.xmu)\n",
    "        self.var = np.mean(self.sq, axis=0) #(D,)\n",
    "        \n",
    "        # 计算归一化之后的x\n",
    "        self.sqrtvar = np.sqrt(self.var + self.eps)  # (D, )\n",
    "        self.ivar = 1.0 / self.sqrtvar\n",
    "        self.xhat = self.xmu * self.ivar # （N, D)\n",
    "        self.gammax = self.gamma * self.xhat\n",
    "\n",
    "        # 输出\n",
    "        y = self.gammax + self.beta # (N, D)\n",
    "        return y\n",
    "    def backward(self, dout):\n",
    "        self.dbeta = np.sum(dout, axis=0) #(D,)\n",
    "        self.dgamma = np.sum(dout * self.xhat, axis=0) #(D,)\n",
    "        \n",
    "        # 计算dx,计算步骤比较长，需要一步一步进行计算，最后使用链式法则组装\n",
    "        N, D = self.xhat.shape\n",
    "        dy = dout * 1 #(N, D)\n",
    "        dgammax = dy * 1\n",
    "        dxhat = dgammax * self.gamma #(N, D)\n",
    "        dxmu1 = dxhat * self.ivar # (N, D) \n",
    "        \n",
    "        divar = np.sum(dxhat * self.xmu, axis=0) # (D,)\n",
    "        dsqrtvar = divar * (-1.0 / np.square(self.sqrtvar)) #(D,)\n",
    "        dvar = dsqrtvar * (1.0 / (2*self.sqrtvar))  #(D,)\n",
    "        dsq = dvar * (np.ones((N, D)) * 1.0 / N) #(N, D)\n",
    "        \n",
    "        dxmu2 = dsq * 2 * self.xmu # (N, D) \n",
    "        \n",
    "        dxmu = dxmu1 + dxmu2 # (N, D)\n",
    "        dx1 = dxmu # (N, D) \n",
    "        \n",
    "        dmu = np.sum(dxmu * (-1), axis=0) #(, D)\n",
    "        \n",
    "        dx2 = dmu * (np.ones((N, D))*1.0/N) #(N, D)\n",
    "        \n",
    "        dx = dx1 + dx2 #(N, D)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = batch_normal(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = bn.backward(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "print dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "\n",
    "    N, D = x.shape\n",
    "\n",
    "    #step1: calculate mean\n",
    "    mu = 1./N * np.sum(x, axis = 0)\n",
    "\n",
    "    #step2: subtract mean vector of every trainings example\n",
    "    xmu = x - mu\n",
    "\n",
    "    #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "\n",
    "    #step4: calculate variance\n",
    "    var = 1./N * np.sum(sq, axis = 0)\n",
    "\n",
    "    #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "    #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "\n",
    "    #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "\n",
    "    #step8: Nor the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "\n",
    "    #step9\n",
    "    out = gammax + beta\n",
    "\n",
    "    #store intermediate\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "\n",
    "    #unfold the variables stored in cache\n",
    "    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "\n",
    "    #get the dimensions of the input/output\n",
    "    N,D = dout.shape\n",
    "\n",
    "    #step9\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "    #step8\n",
    "    dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "    dxhat = dgammax * gamma\n",
    "\n",
    "    #step7\n",
    "    divar = np.sum(dxhat*xmu, axis=0)\n",
    "    dxmu1 = dxhat * ivar\n",
    "\n",
    "    #step6\n",
    "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "    #step5\n",
    "    dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "    #step4\n",
    "    dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "\n",
    "    #step3\n",
    "    dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "    #step2\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "\n",
    "    #step1\n",
    "    dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "\n",
    "    #step0\n",
    "    dx = dx1 + dx2\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, cache = batchnorm_forward(x, 1, 0, 10e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddx, dgamma, dbeta = batchnorm_backward(dout, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.dgamma == dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbeta == bn.dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx[0] == ddx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "- 为了抑制过拟合，给损失函数添加正的正则化项来使权重减小的方法，包括L1、L2两种方法，L2正则化可使权重全部减小，L1正则化则能稀疏化权重，使产生更多的权重值为0\n",
    "- 权重衰减项即是每层 **W** 的线性加和，在最终的反向传播的过程中，只需要在 **dW** 项中加上 **lambda x W**即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout层\n",
    "- 为了抑制过拟合，DropOut层在每层随机选择部分神经元使之失活，输出为0，达到降低模型复杂度，从而减小过拟合现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(object):\n",
    "    def __init__(self, dropout_radio = 0.4):\n",
    "        self.dropout_radio = dropout_radio\n",
    "        self.mask = None\n",
    "    def forward(self, x, train_sign=True):\n",
    "        if train_sign == True:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_radio\n",
    "            return self.mask * x\n",
    "        else:\n",
    "            return x * (1 - self.dropout_radio)  # 为啥要这样处理？\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
