{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "- C4.5算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    # 根据标签序列计算信息熵\n",
    "    def get_Information_entropy(self, labels_list, n_samples):\n",
    "        \"\"\"\n",
    "        :labels_list, ndarray, 标签列表、数组\n",
    "        :n_samples, int, 总的类别数\n",
    "        计算信息熵\n",
    "        \"\"\"\n",
    "        _, label_counts = np.unique(labels_list, return_counts=True)\n",
    "        p = label_counts * 1.0 / n_samples\n",
    "        return -np.sum(p * np.log2(p))\n",
    "    # 根据标签类别数计算信息熵\n",
    "    def get_Information_entropy_with_counts(self, labels_count, n_samples):\n",
    "        \"\"\"\n",
    "        :labels_count, ndarray, 标签数量列表、数组\n",
    "        :n_samples, int, 总的标签数\n",
    "        计算信息熵\n",
    "        \"\"\"\n",
    "        p = labels_count * 1.0 / n_samples\n",
    "        return -np.sum(p * np.log2(p))\n",
    "\n",
    "    def __init__(self, data_x, data_y, segmentation_attr, attr_is_dispersed):\n",
    "        '''\n",
    "        :data_x: ndarray, 标签化的数据集x\n",
    "        :data_y: ndarray, 标签化的数据集y\n",
    "        :segmentation_attr, list, 当前可用的分割属性下标列表，例如[1,5,7],表示对于当前节点只有1、5、7可以\n",
    "        被用作属性分割\n",
    "        :attr_is_dispersed, ndarray, 属性是否为离散的，例如[1,0,1,0,1],则表示下标0、2、4的属性是离散的，\n",
    "        '''\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.segmentation_attr = segmentation_attr\n",
    "        self.next_nodes = {}  # 存储子节点\n",
    "        self.is_leaf = None\n",
    "        self.seg_attr_index = -1  # 在该节点选择的分割属性\n",
    "        self.seg_attr_value = -1  # 分割属性值，如果是离散的，则返回属性唯一值序列，否则返回只包含一\n",
    "        # 个分割点属性值的序列\n",
    "\n",
    "        n_samples, n_features = data_x.shape\n",
    "        # 当前节点数据数不大于1、无可用分割属性、数据标签y全部一致的情况下，认定为子节点\n",
    "        uniques_y = np.unique(self.data_y, return_counts=False)\n",
    "        if n_samples <= 1 or len(self.segmentation_attr) == 0 or len(uniques_y) == 1:\n",
    "            self.is_leaf = True\n",
    "        else:  # 非叶子节点\n",
    "            gain = self.get_Information_entropy(self.data_y, n_samples)  # 熵\n",
    "            self.is_leaf = False\n",
    "            # 根据可用分割属性segmentation_attr，以及属性离散/连续记录表attr_is_dispersed来确定最优\n",
    "            # 分割属性,默认采用信息增益的方式，ID3算法\n",
    "            temp_gain = -1  # 保存最小的条件熵\n",
    "            temp_attr_index = -1  # 保存按哪个属性分割可以得到最小的条件熵\n",
    "\n",
    "            # 保存该属性分割点的值，如果是离散的，则返回的列表包括属性的唯一值序列，否则仅\n",
    "            # 返回包含分割点的属性值\n",
    "            temp_attr_seg = None\n",
    "            # 如果优属性是连续的，则返回升序属性序列的分割点下标，方便后续计算\n",
    "            temp_continus_attr_seg_index = -1\n",
    "            for attr_index in self.segmentation_attr:\n",
    "                if attr_is_dispersed[attr_index] == 1:  # 离散值\n",
    "                    uniques_attr, uniques_attr_counts = np.unique(\n",
    "                        self.data_x[:, attr_index], return_counts=True)\n",
    "                    temp_information_gains = []\n",
    "                    for cur_attr_label in uniques_attr:\n",
    "                        cur_mask = (self.data_x[:, attr_index] == cur_attr_label)\n",
    "                        temp_information_gains.append(\n",
    "                            self.get_Information_entropy(self.data_y[cur_mask], len(cur_mask)))\n",
    "                    cur_gain = np.sum((uniques_attr_counts * 1.0 / n_samples) *\n",
    "                                      temp_information_gains)\n",
    "                    \n",
    "                    gain_cur_attr = self.get_Information_entropy_with_counts(\n",
    "                        uniques_attr_counts, n_samples)\n",
    "                    if temp_gain < (gain - cur_gain) / gain_cur_attr:\n",
    "                        temp_gain = (gain - cur_gain) / gain_cur_attr\n",
    "                        temp_attr_index = attr_index\n",
    "                        temp_attr_seg = uniques_attr    \n",
    "                    \n",
    "                else:  # 连续值,需要寻找一个最优的二分点来分割数据，需要进行n_samples-1次尝试，\n",
    "                    # 对于ID3算法，最优二分点处条件熵最小即可，对于C4.5，则需要计算归一化的信息\n",
    "                    # 增益，来决定连续属性的最优二分割点\n",
    "                    sort_index = np.argsort(self.data_x[:, attr_index])\n",
    "                    temp_continus_attr_seg = -1\n",
    "                    temp_continus_gain = -1\n",
    "                    temp_sort_index = -1\n",
    "                    for i in range(n_samples - 1):\n",
    "                        temp_continus_seg_value = (data_x[i, attr_index] + \\\n",
    "                                                   data_x[i + 1, attr_index]) / 2.0\n",
    "                        temp_continuous_left = self.get_Information_entropy(\n",
    "                            data_y[sort_index[:i + 1]], i + 1)\n",
    "                        temp_continuous_right = self.get_Information_entropy(\n",
    "                            data_y[sort_index[i + 1:]], n_samples - i - 1)\n",
    "                        # 计算连续属性的条件熵\n",
    "                        cur_continus_gains = (i + 1.0) / n_samples * temp_continuous_left + \\\n",
    "                                             (n_samples - i - 1.0) / n_samples * temp_continuous_right\n",
    "                        \n",
    "                        # 计算当前二分点处的固有值， 在不同的二分割点处，该属性具有不同的“固有值”\n",
    "                        cur_seg_iv = self.get_Information_entropy_with_counts(\n",
    "                        np.array([i+1,n_samples - i - 1]), n_samples)\n",
    "                        if temp_continus_gain < (gain - cur_continus_gains)/cur_seg_iv:\n",
    "                            temp_continus_gain = (gain - cur_continus_gains)/cur_seg_iv\n",
    "                            temp_continus_attr_seg = temp_continus_seg_value\n",
    "                            temp_sort_index = i\n",
    "                            \n",
    "                    cur_gain = temp_continus_gain\n",
    "                    if temp_gain < cur_gain:\n",
    "                        temp_gain = cur_gain\n",
    "                        temp_attr_index = attr_index\n",
    "                        temp_attr_seg = np.array([temp_continus_attr_seg])\n",
    "                        temp_continus_attr_seg_index = temp_sort_index\n",
    "                            \n",
    "            # 利用最优属性进行划分，并创造该节点的子节点，保存在self.next_nodes结构中\n",
    "\n",
    "            self.seg_attr_index = temp_attr_index  # 该节点的分割属性（轴）的下标\n",
    "            self.seg_attr_value = temp_attr_seg\n",
    "\n",
    "            # 最优属性是离散值\n",
    "            if attr_is_dispersed[temp_attr_index] == 1:\n",
    "                self.segmentation_attr.remove(temp_attr_index)  # 从备用分割属性列表中删除最优属性\n",
    "                for cur_attr_label in temp_attr_seg:\n",
    "                    cur_mask = (self.data_x[:, self.seg_attr_index] == cur_attr_label)\n",
    "                    self.next_nodes[cur_attr_label] = TreeNode(self.data_x[cur_mask],\n",
    "                                                               self.data_y[cur_mask],\n",
    "                                                               self.segmentation_attr,\n",
    "                                                               attr_is_dispersed)\n",
    "            else:  # 最优属性是连续值\n",
    "                sort_index = np.argsort(self.data_x[:, temp_attr_index])\n",
    "                # 不大于分割属性的子节点\n",
    "                self.next_nodes[0] = TreeNode(\n",
    "                    data_x[sort_index[:temp_continus_attr_seg_index + 1]],\n",
    "                    data_y[sort_index[:temp_continus_attr_seg_index + 1]],\n",
    "                    self.segmentation_attr,\n",
    "                    attr_is_dispersed)  # left\n",
    "                # 大于分割属性的子节点\n",
    "                self.next_nodes[1] = TreeNode(\n",
    "                    data_x[sort_index[temp_continus_attr_seg_index + 1:]],\n",
    "                    data_y[sort_index[temp_continus_attr_seg_index + 1:]],\n",
    "                    self.segmentation_attr,\n",
    "                    attr_is_dispersed)  # right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, train_x, train_y, attributes_classs=None,\n",
    "                 criterion='entropy'):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "        # 确定属性是连续/离散的,当属性中唯一值数量多于N/2时，认定为连续值\n",
    "        n_samples, n_features = self.train_x.shape\n",
    "        if attributes_classs is None:\n",
    "            attributes_classs = [0] * n_samples\n",
    "            for i in range(n_features):\n",
    "                uniques_i = np.unique(self.train_x[:, i])\n",
    "                if uniques_i * 3 > n_samples:\n",
    "                    attributes_classs[i] = 0\n",
    "        self.attributes_classs = attributes_classs\n",
    "        # 对离散属性、label做encoder处理\n",
    "        self.xLabelEncoders = []\n",
    "        for i in range(len(self.attributes_classs)):\n",
    "            if self.attributes_classs[i] == 1:  # 离散属性\n",
    "                cur_encoder = LabelEncoder()\n",
    "                cur_encoder.fit(self.train_x[:, i])\n",
    "                self.train_x[:, i] = cur_encoder.transform(self.train_x[:, i])\n",
    "                self.xLabelEncoders.append(cur_encoder)\n",
    "            else:\n",
    "                self.xLabelEncoders.append(None)\n",
    "        self.yLabelEncoders = LabelEncoder()\n",
    "        self.yLabelEncoders.fit(train_y)\n",
    "        self.train_y = self.yLabelEncoders.transform(self.train_y)\n",
    "        self.root = None  # 根节点\n",
    "\n",
    "    def train(self):\n",
    "        self.root = TreeNode(self.train_x, self.train_y, range(len(self.attributes_classs)),\n",
    "                             self.attributes_classs)\n",
    "\n",
    "    def fit(self):\n",
    "        self.train()\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        # 从根节点开始遍历树形结构，\n",
    "        if self.root is None:\n",
    "            raise RuntimeError(\"value is None, error\")\n",
    "        # 首先将测试数据集的离散属性LAbelencoder,然后进行预测\n",
    "        for (i, x_label_encoder) in enumerate(self.xLabelEncoders):\n",
    "            if x_label_encoder is not None:\n",
    "                test_x[:, i] = x_label_encoder.transform(test_x[:, i],)\n",
    "        n_samples, _ = test_x.shape\n",
    "        pre = np.zeros(n_samples)\n",
    "        for i in xrange(n_samples):\n",
    "            cur_node = self.root\n",
    "            while cur_node.is_leaf is False:\n",
    "                cur_attr_index = cur_node.seg_attr_index  # 分割属性下标\n",
    "                if self.attributes_classs[cur_attr_index] == 1:  # 离散属性\n",
    "                    if test_x[i][cur_attr_index] in cur_node.next_nodes.keys():\n",
    "                        cur_node = cur_node.next_nodes[test_x[i][cur_attr_index]]\n",
    "                        break\n",
    "                    else:\n",
    "                        raise Exception('the attr is not in Decision Tree')\n",
    "                else:  # 连续属性\n",
    "                    if test_x[i][cur_attr_index] <= cur_node.seg_attr_value:\n",
    "                        cur_node = cur_node.next_nodes[0]\n",
    "                    else:\n",
    "                        cur_node = cur_node.next_nodes[1]\n",
    "            # 找到了叶子节点，可以展开预测动作了，基本的投票原则\n",
    "            pre[i] = np.argmax(np.bincount(cur_node.data_y))\n",
    "        pre = pre.astype(np.int)\n",
    "        return self.yLabelEncoders.inverse_transform(pre)  # 反向转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path = '../data/heart.csv'):\n",
    "    weather = pd.read_csv(path,sep=',')\n",
    "    data = weather.values\n",
    "    data_x = data[:,:-1]\n",
    "    data_y = data[:, -1]\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(train_x, train_y, [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hao/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "dt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_y = dt.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65573770491803274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pre_y == test_y)*1.0 / test_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
