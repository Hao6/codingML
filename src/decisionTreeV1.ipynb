{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "- 首先实现用于分类任务的决策树，需要对数据进行预处理：\n",
    "    1. 对label进行编码，由sklearn.LabelEncoder实现，在预测时将结果反编码\n",
    "    2. 对离散属性编码，\n",
    "    3. 对连续属性进行分段处理转化为category类型，然后编码\n",
    "    4. 使用嵌套字典形式存储树结构\n",
    "    5. 通过信息熵计算信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine, load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    def get_Information_entropy(self, labels_list, n_samples):\n",
    "        \"\"\"\n",
    "        :labels_list, ndarray, 标签列表、数组\n",
    "        :n_samples, int, 总的类别数\n",
    "        计算信息熵\n",
    "        \"\"\"\n",
    "        _, label_counts = np.unique(labels_list, return_counts=True)\n",
    "        p = label_counts * 1.0 / n_samples\n",
    "        return -np.sum(p * np.log2(p))\n",
    "\n",
    "    def __init__(self, data_x, data_y, segmentation_attr, attr_is_dispersed):\n",
    "        '''\n",
    "        :data_x: ndarray, 标签化的数据集x\n",
    "        :data_y: ndarray, 标签化的数据集y\n",
    "        :segmentation_attr, list, 当前可用的分割属性下标列表，例如[1,5,7],表示对于当前节点只有1、5、7可以\n",
    "        被用作属性分割\n",
    "        :attr_is_dispersed, ndarray, 属性是否为离散的，例如[1,0,1,0,1],则表示下标0、2、4的属性是离散的，\n",
    "        '''\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.segmentation_attr = segmentation_attr\n",
    "        self.next_nodes = {}  # 存储子节点\n",
    "        self.is_leaf = None\n",
    "        self.seg_attr_index = -1  # 在该节点选择的分割属性\n",
    "        self.seg_attr_value = -1  # 分割属性值，如果是离散的，则返回属性唯一值序列，否则返回只包含一\n",
    "        # 个分割点属性值的序列\n",
    "\n",
    "        n_samples, n_features = data_x.shape\n",
    "        # 当前节点数据数不大于10、无可用分割属性、数据标签y全部一致的情况下，认定为子节点\n",
    "        uniques_y = np.unique(self.data_y, return_counts=False)\n",
    "        if n_samples <= 1 or len(self.segmentation_attr) == 0 or len(uniques_y) == 1:\n",
    "            self.is_leaf = True\n",
    "        else:  # 非叶子节点\n",
    "            gain = self.get_Information_entropy(self.data_y, n_samples)  # 熵\n",
    "            self.is_leaf = False\n",
    "            # 根据可用分割属性segmentation_attr，以及属性离散/连续记录表attr_is_dispersed来确定最优\n",
    "            # 分割属性,默认采用信息增益的方式，ID3算法\n",
    "            temp_gain = -1  # 保存最小的条件熵\n",
    "            temp_attr_index = -1  # 保存按哪个属性分割可以得到最小的条件熵\n",
    "\n",
    "            # 保存该属性分割点的值，如果是离散的，则返回的列表包括属性的唯一值序列，否则仅\n",
    "            # 返回包含分割点的属性值\n",
    "            temp_attr_seg = None\n",
    "            # 如果优属性是连续的，则返回升序属性序列的分割点下标，方便后续计算\n",
    "            temp_continus_attr_seg_index = -1\n",
    "            for attr_index in self.segmentation_attr:\n",
    "                if attr_is_dispersed[attr_index] == 1:  # 离散值\n",
    "                    uniques_attr, uniques_attr_counts = np.unique(\n",
    "                        self.data_x[:, attr_index], return_counts=True)\n",
    "                    temp_information_gains = []\n",
    "                    for cur_attr_label in uniques_attr:\n",
    "                        cur_mask = (self.data_x[:, attr_index] == cur_attr_label)\n",
    "                        temp_information_gains.append(\n",
    "                            self.get_Information_entropy(self.data_y[cur_mask], np.sum(cur_mask)))\n",
    "                    cur_gain = np.sum((uniques_attr_counts * 1.0 / n_samples) *\n",
    "                                      temp_information_gains)\n",
    "                    if temp_gain < (gain - cur_gain): # id3，信息增益\n",
    "                        temp_gain = (gain - cur_gain)\n",
    "                        temp_attr_index = attr_index\n",
    "                        temp_attr_seg = uniques_attr\n",
    "                else:  # 连续值,需要寻找一个最优的二分点来分割数据，需要进行n_samples-1次尝试\n",
    "                    sort_index = np.argsort(self.data_x[:, attr_index])\n",
    "                    temp_continus_attr_seg = -1\n",
    "                    temp_continus_gain = np.inf\n",
    "                    temp_sort_index = -1\n",
    "                    for i in range(n_samples - 1):\n",
    "                        temp_continus_seg_value = (data_x[i, attr_index] + \\\n",
    "                                                   data_x[i + 1, attr_index]) / 2.0\n",
    "                        temp_continuous_left = self.get_Information_entropy(\n",
    "                            data_y[sort_index[:i + 1]], i + 1)\n",
    "                        temp_continuous_right = self.get_Information_entropy(\n",
    "                            data_y[sort_index[i + 1:]], n_samples - i - 1)\n",
    "                        # 计算连续属性的条件熵\n",
    "                        cur_continus_gains = (i + 1.0) / n_samples * temp_continuous_left + \\\n",
    "                                             (n_samples - i - 1.0) / n_samples * temp_continuous_right\n",
    "                        if temp_continus_gain > cur_continus_gains:\n",
    "                            temp_continus_gain = cur_continus_gains\n",
    "                            temp_continus_attr_seg = temp_continus_seg_value\n",
    "                            temp_sort_index = i\n",
    "                        cur_gain = temp_continus_gain\n",
    "                        if temp_gain < (gain - cur_gain):  # id3，信息增益\n",
    "                            temp_gain = (gain - cur_gain)\n",
    "                            temp_attr_index = attr_index\n",
    "                            temp_attr_seg = np.array([temp_continus_attr_seg])\n",
    "                            temp_continus_attr_seg_index = temp_sort_index\n",
    "            # 利用最优属性进行划分，并创造该节点的子节点，保存在self.next_nodes结构中\n",
    "\n",
    "            self.seg_attr_index = temp_attr_index  # 该节点的分割属性（轴）的下标\n",
    "            self.seg_attr_value = temp_attr_seg\n",
    "\n",
    "            # 最优属性是离散值\n",
    "            if attr_is_dispersed[temp_attr_index] == 1:\n",
    "                self.segmentation_attr.remove(temp_attr_index)  # 从备用分割属性列表中删除最优属性\n",
    "                for cur_attr_label in temp_attr_seg:\n",
    "                    cur_mask = (self.data_x[:, self.seg_attr_index] == cur_attr_label)\n",
    "                    self.next_nodes[cur_attr_label] = TreeNode(self.data_x[cur_mask],\n",
    "                                                               self.data_y[cur_mask],\n",
    "                                                               self.segmentation_attr,\n",
    "                                                               attr_is_dispersed)\n",
    "            else:  # 最优属性是连续值\n",
    "                sort_index = np.argsort(self.data_x[:, temp_attr_index])\n",
    "                # 不大于分割属性的子节点\n",
    "                self.next_nodes[0] = TreeNode(\n",
    "                    data_x[sort_index[:temp_continus_attr_seg_index + 1]],\n",
    "                    data_y[sort_index[:temp_continus_attr_seg_index + 1]],\n",
    "                    self.segmentation_attr,\n",
    "                    attr_is_dispersed)  # left\n",
    "                # 大于分割属性的子节点\n",
    "                self.next_nodes[1] = TreeNode(\n",
    "                    data_x[sort_index[temp_continus_attr_seg_index + 1:]],\n",
    "                    data_y[sort_index[temp_continus_attr_seg_index + 1:]],\n",
    "                    self.segmentation_attr,\n",
    "                    attr_is_dispersed)  # right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, train_x, train_y, attributes_classs=None):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "\n",
    "        # 确定属性是连续/离散的,当属性中唯一值数量多于N/2时，认定为连续值\n",
    "        n_samples, n_features = self.train_x.shape\n",
    "        if attributes_classs is None:\n",
    "            attributes_classs = [0] * n_samples\n",
    "            for i in range(n_features):\n",
    "                uniques_i = np.unique(self.train_x[:, i])\n",
    "                if uniques_i * 3 > n_samples:\n",
    "                    attributes_classs[i] = 0\n",
    "        self.attributes_classs = attributes_classs\n",
    "        # 对离散属性、label做encoder处理\n",
    "        self.xLabelEncoders = []\n",
    "        for i in range(len(self.attributes_classs)):\n",
    "            if self.attributes_classs[i] == 1:  # 离散属性\n",
    "                cur_encoder = LabelEncoder()\n",
    "                cur_encoder.fit(self.train_x[:, i])\n",
    "                self.train_x[:, i] = cur_encoder.transform(self.train_x[:, i])\n",
    "                self.xLabelEncoders.append(cur_encoder)\n",
    "            else:\n",
    "                self.xLabelEncoders.append(None)\n",
    "        self.yLabelEncoders = LabelEncoder()\n",
    "        self.yLabelEncoders.fit(train_y)\n",
    "        self.train_y = self.yLabelEncoders.transform(self.train_y)\n",
    "        self.root = None  # 根节点\n",
    "\n",
    "    def train(self):\n",
    "        self.root = TreeNode(self.train_x, self.train_y, range(len(self.attributes_classs)),\n",
    "                             self.attributes_classs)\n",
    "\n",
    "    def fit(self):\n",
    "        self.train()\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        # 从根节点开始遍历树形结构，\n",
    "        if self.root is None:\n",
    "            raise RuntimeError(\"value is None, error\")\n",
    "        # 首先将测试数据集的离散属性LAbelencoder,然后进行预测\n",
    "        for (i, x_label_encoder) in enumerate(self.xLabelEncoders):\n",
    "            if x_label_encoder is not None:\n",
    "                test_x[:, i] = x_label_encoder.transform(test_x[:, i])\n",
    "        n_samples, _ = test_x.shape\n",
    "        pre = np.zeros(n_samples)\n",
    "        for i in xrange(n_samples):\n",
    "            cur_node = self.root\n",
    "            while cur_node.is_leaf is False:\n",
    "                cur_attr_index = cur_node.seg_attr_index  # 分割属性下标\n",
    "                if self.attributes_classs[cur_attr_index] == 1:  # 离散属性\n",
    "                    if test_x[i][cur_attr_index] in cur_node.next_nodes.keys():\n",
    "                        cur_node = cur_node.next_nodes[test_x[i][cur_attr_index]]\n",
    "                        break\n",
    "                    else:\n",
    "                        raise Exception('the attr is not in Decision Tree')\n",
    "                else:  # 连续属性\n",
    "                    if test_x[i][cur_attr_index] <= cur_node.seg_attr_value:\n",
    "                        cur_node = cur_node[0]\n",
    "                    else:\n",
    "                        cur_node = cur_node[1]\n",
    "            # 找到了叶子节点，可以展开预测动作了，基本的投票原则\n",
    "            pre[i] = np.argmax(np.bincount(cur_node.data_y))\n",
    "        pre = pre.astype(np.int)\n",
    "        return self.yLabelEncoders.inverse_transform(pre)  # 反向装换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遍历决策树\n",
    "- 作为绘制决策树的基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树的遍历，以字典的形式返回\n",
    "def list_dt(root, attr_is_dispersed):\n",
    "    if root.is_leaf is True:\n",
    "        return np.argmax(np.bincount(root.data_y)) # 返回预测标签\n",
    "    else:\n",
    "        temp_dic = {}\n",
    "        if attr_is_dispersed[root.seg_attr_index] == 1:\n",
    "            for next_node in root.next_nodes.keys():\n",
    "                temp_dic[str(root.seg_attr_index)+' is '+str(next_node)] = list_dt(root.next_nodes[next_node],\n",
    "                                                                                 attr_is_dispersed)\n",
    "        else:\n",
    "            temp_dic[str(root.seg_attr_index)+' <= ' + str(root.seg_attr_value)] = list_dt(root.next_nodes[0],\n",
    "                                                                                         attr_is_dispersed)\n",
    "            temp_dic[str(root.seg_attr_index)+' > ' + str(root.seg_attr_value)] = list_dt(root.next_nodes[1],\n",
    "                                                                                        attr_is_dispersed)\n",
    "        return temp_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path = '../data/heart.csv'):\n",
    "    weather = pd.read_csv(path,sep=',')\n",
    "    data = weather.values\n",
    "    data_x = data[:,:-1]\n",
    "    data_y = data[:, -1]\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mask = [1, 5, 8, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data_x[:,features_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_classs = np.array([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(train_x, train_y, attributes_classs[features_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_y =  dt.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集正确率：0.754098\n"
     ]
    }
   ],
   "source": [
    "print '测试集正确率：%f' % (np.sum(pre_y == test_y) * 1.0 / test_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_y = dt.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集正确率：0.768595\n"
     ]
    }
   ],
   "source": [
    "print '训练集正确率：%f' % (np.sum(pre_train_y == train_y) * 1.0 / train_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_dt = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_dt.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_pre = sk_dt.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72131147541\n"
     ]
    }
   ],
   "source": [
    "print np.sum(sk_pre == test_y)*1.0 / test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_train_pre = sk_dt.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789256198347\n"
     ]
    }
   ],
   "source": [
    "print np.sum(sk_train_pre == train_y)*1.0 / train_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named graphviz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-f7133d3517ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named graphviz"
     ]
    }
   ],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
