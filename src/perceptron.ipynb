{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知器（单层感知器） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_inputs=2):\n",
    "        self.W = np.random.randn(n_inputs,1)\n",
    "        self.b = np.random.randn(1)\n",
    "    def forward(self, x):\n",
    "        # 实际上相当于使用了阶跃函数作为激活函数\n",
    "        return ((np.matmul(x.reshape((1,-1)),self.W.reshape((-1,1)))+self.b) > 0).astype(np.int32)\n",
    "    \n",
    "    def backward(self, x, label, pre, alpha=0.05):\n",
    "        self.W = self.W + alpha * (label - pre) * x.reshape(*self.W.shape)\n",
    "        self.b -= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[0,0],[0,1],[1,0],[1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与门 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Andgate(Perceptron):\n",
    "    def __init__(self):\n",
    "        self.W = np.array([[0.5],[0.5]])\n",
    "        self.b = -0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] 0\n",
      "[0 1] 0\n",
      "[1 0] 0\n",
      "[1 1] 1\n"
     ]
    }
   ],
   "source": [
    "and_gate = Andgate()\n",
    "for x in data_x:\n",
    "    print x,and_gate.forward(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 或门 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orgate(Perceptron):\n",
    "    def __init__(self):\n",
    "        self.W = np.array([[0.5],[0.5]])\n",
    "        self.b = -0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] 0\n",
      "[0 1] 1\n",
      "[1 0] 1\n",
      "[1 1] 1\n"
     ]
    }
   ],
   "source": [
    "or_gate = Orgate()\n",
    "for x in data_x:\n",
    "    print x,or_gate.forward(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与非门 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nandgate(Perceptron):\n",
    "    def __init__(self):\n",
    "        self.W = np.array([[-0.5],[-0.5]])\n",
    "        self.b = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] 1\n",
      "[0 1] 1\n",
      "[1 0] 1\n",
      "[1 1] 0\n"
     ]
    }
   ],
   "source": [
    "nand_gate = Nandgate()\n",
    "for x in data_x:\n",
    "    print x,nand_gate.forward(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异或门 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Xorgate(Perceptron):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        and_gate = Andgate()\n",
    "        or_gate = Orgate()\n",
    "        nand_gate = Nandgate()\n",
    "        \n",
    "        or_res = or_gate.forward(x)\n",
    "        nand_res = nand_gate.forward(x)\n",
    "        return and_gate.forward(np.array([or_res, nand_res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] 0\n",
      "[0 1] 1\n",
      "[1 0] 1\n",
      "[1 1] 0\n"
     ]
    }
   ],
   "source": [
    "xor_gate = Xorgate()\n",
    "for x in data_x:\n",
    "    print x,xor_gate.forward(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知器学习 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知器学习的策略是最小化如下的损失函数\n",
    "$$L_{(W,b)} = -\\sum_{X_{i} \\in M}y_{i}(WX_{i}+b)$$\n",
    "其中M表示错误分类的实例集合，$WX_{i}+b$表示$X_{i}$到超平面$WX+b=0$的距离，且该距离与$y_{i}$符号相反，所以需要添加一个`-`号；为了最小化这个损失函数为0，即所有实例均被正确分类，我们采用SGD来进行最小化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器学习算法的原始形式 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_single(object):\n",
    "    def __init__(self, n_inputs=2):\n",
    "        self.W = np.random.randn(n_inputs, 1)\n",
    "        self.b = np.random.randn(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        t = self.forward(x)\n",
    "        p = np.where(t > 0, 1, -1)\n",
    "        mid = np.abs((y - p))*y / 2.0\n",
    "        losses = -np.dot(mid.T, t)\n",
    "        return losses\n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        t = np.where(self.forward(x) > 0, 1, -1)\n",
    "        return (1.0 * np.sum(t == y)) / y.shape[0]\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        t = self.forward(x)\n",
    "        p = np.where(t > 0, 1, -1)\n",
    "        mid = np.abs((p - y))*y / 2.0\n",
    "        dW = -np.dot(x.T, mid)\n",
    "        db = -np.sum(mid)\n",
    "        return (dW, db)\n",
    "\n",
    "def train(datas_x, datas_y, alpha=0.01, epochs=100, tests_x=None, tests_y=None, batch_size=100):\n",
    "    net = Perceptron_single(n_inputs=datas_x.shape[1])\n",
    "    train_loss_list = []\n",
    "    train_size = datas_x.shape[0]\n",
    "    for i in range(epochs):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x = datas_x[batch_mask]\n",
    "        y = datas_y[batch_mask] # 抽样\n",
    "        grad = net.gradient(x, y)\n",
    "        \n",
    "        net.W -= grad[0]\n",
    "        net.b -= grad[1]\n",
    "        loss = net.loss(x, y)\n",
    "        print \"has trained %d times, the train's loss %f\" % (i+1, loss)\n",
    "        if tests_x is not None and tests_y is not None:\n",
    "            print net.accuracy(tests_x, tests_y)\n",
    "        train_loss_list.append(loss)\n",
    "    return net, train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pers_data():\n",
    "    W0 = 2.5\n",
    "    W1 = -0.5\n",
    "    b = -7.5\n",
    "    \n",
    "    x0 = np.random.randint(-200, 100, 10000)\n",
    "    x1 = np.random.randint(-67, 209, 10000)\n",
    "    y = np.where(((W0 * x0 + W1 * x1 + b) - 3.6 * np.random.randn(10000) + 13) > 0, 1, -1)\n",
    "    return np.array([x0, x1]).T, y.reshape(1, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "[[-37 155]\n",
      " [-87 -29]\n",
      " [ 98 122]\n",
      " [-97 170]\n",
      " [-19 175]]\n",
      "[[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]\n",
      " [-1]]\n"
     ]
    }
   ],
   "source": [
    "x, y = load_pers_data()\n",
    "print x.shape\n",
    "print x[:5]\n",
    "print y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2923,)\n",
      "(7077,)\n"
     ]
    }
   ],
   "source": [
    "y_1 = y[y == 1]\n",
    "y_0 = y[y == -1]\n",
    "print y_1.shape\n",
    "print y_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_x = x[8000:]\n",
    "tests_y = y[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_x = x[:8000]\n",
    "trains_y = y[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has trained 1 times, the train's loss 5713990.670348\n",
      "0.838\n",
      "has trained 2 times, the train's loss 2537047.171514\n",
      "0.869\n",
      "has trained 3 times, the train's loss 987054.216512\n",
      "0.906\n",
      "has trained 4 times, the train's loss 502597.914359\n",
      "0.9475\n",
      "has trained 5 times, the train's loss -0.000000\n",
      "0.957\n",
      "has trained 6 times, the train's loss 15131.235517\n",
      "0.988\n",
      "has trained 7 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 8 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 9 times, the train's loss 37805.765933\n",
      "0.991\n",
      "has trained 10 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 11 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 12 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 13 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 14 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 15 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 16 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 17 times, the train's loss 870.301162\n",
      "0.9935\n",
      "has trained 18 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 19 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 20 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 21 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 22 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 23 times, the train's loss 5057.532948\n",
      "0.9935\n",
      "has trained 24 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 25 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 26 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 27 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 28 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 29 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 30 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 31 times, the train's loss 4539.125635\n",
      "0.9935\n",
      "has trained 32 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 33 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 34 times, the train's loss 9517.690192\n",
      "0.9935\n",
      "has trained 35 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 36 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 37 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 38 times, the train's loss 1881.416700\n",
      "0.9935\n",
      "has trained 39 times, the train's loss 5449.357561\n",
      "0.993\n",
      "has trained 40 times, the train's loss 10572.025279\n",
      "0.993\n",
      "has trained 41 times, the train's loss 26273.317055\n",
      "0.9925\n",
      "has trained 42 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 43 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 44 times, the train's loss 3237.502709\n",
      "0.994\n",
      "has trained 45 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 46 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 47 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 48 times, the train's loss -0.000000\n",
      "0.9915\n",
      "has trained 49 times, the train's loss 13827.273980\n",
      "0.9935\n",
      "has trained 50 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 51 times, the train's loss 6438.498630\n",
      "0.992\n",
      "has trained 52 times, the train's loss -0.000000\n",
      "0.992\n",
      "has trained 53 times, the train's loss 7298.371626\n",
      "0.9915\n",
      "has trained 54 times, the train's loss 96201.448795\n",
      "0.9925\n",
      "has trained 55 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 56 times, the train's loss 814.248701\n",
      "0.994\n",
      "has trained 57 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 58 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 59 times, the train's loss -0.000000\n",
      "0.99\n",
      "has trained 60 times, the train's loss 24358.317055\n",
      "0.9945\n",
      "has trained 61 times, the train's loss 18647.466281\n",
      "0.9935\n",
      "has trained 62 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 63 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 64 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 65 times, the train's loss 44773.577711\n",
      "0.9935\n",
      "has trained 66 times, the train's loss 4923.053549\n",
      "0.992\n",
      "has trained 67 times, the train's loss -0.000000\n",
      "0.992\n",
      "has trained 68 times, the train's loss 15262.125635\n",
      "0.994\n",
      "has trained 69 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 70 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 71 times, the train's loss 30233.313716\n",
      "0.9935\n",
      "has trained 72 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 73 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 74 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 75 times, the train's loss 8640.498630\n",
      "0.9945\n",
      "has trained 76 times, the train's loss -0.000000\n",
      "0.9945\n",
      "has trained 77 times, the train's loss -0.000000\n",
      "0.9945\n",
      "has trained 78 times, the train's loss -0.000000\n",
      "0.9945\n",
      "has trained 79 times, the train's loss -0.000000\n",
      "0.9945\n",
      "has trained 80 times, the train's loss 6964.690192\n",
      "0.9935\n",
      "has trained 81 times, the train's loss 26652.589207\n",
      "0.994\n",
      "has trained 82 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 83 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 84 times, the train's loss -0.000000\n",
      "0.994\n",
      "has trained 85 times, the train's loss 16716.087520\n",
      "0.99\n",
      "has trained 86 times, the train's loss -0.000000\n",
      "0.99\n",
      "has trained 87 times, the train's loss 10609.736605\n",
      "0.9935\n",
      "has trained 88 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 89 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 90 times, the train's loss -0.000000\n",
      "0.9935\n",
      "has trained 91 times, the train's loss 13284.125635\n",
      "0.994\n",
      "has trained 92 times, the train's loss 3056.053549\n",
      "0.991\n",
      "has trained 93 times, the train's loss -0.000000\n",
      "0.991\n",
      "has trained 94 times, the train's loss -0.000000\n",
      "0.993\n",
      "has trained 95 times, the train's loss -0.000000\n",
      "0.993\n",
      "has trained 96 times, the train's loss 6920.653904\n",
      "0.9925\n",
      "has trained 97 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 98 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 99 times, the train's loss -0.000000\n",
      "0.9925\n",
      "has trained 100 times, the train's loss -0.000000\n",
      "0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.Perceptron_single at 0x7fbc2218bc10>,\n",
       " [array([[ 5713990.67034782]]),\n",
       "  array([[ 2537047.17151396]]),\n",
       "  array([[ 987054.21651171]]),\n",
       "  array([[ 502597.9143593]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 15131.23551739]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 37805.76593256]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 870.30116222]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 5057.53294815]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 4539.12563472]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 9517.69019159]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 1881.41669983]]),\n",
       "  array([[ 5449.35756114]]),\n",
       "  array([[ 10572.02527893]]),\n",
       "  array([[ 26273.31705475]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 3237.50270885]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 13827.27397952]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 6438.49863011]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 7298.37162598]]),\n",
       "  array([[ 96201.44879535]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 814.24870059]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 24358.31705475]]),\n",
       "  array([[ 18647.46628119]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 44773.57771058]]),\n",
       "  array([[ 4923.05354911]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 15262.12563472]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 30233.31371621]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 8640.49863011]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 6964.69019159]]),\n",
       "  array([[ 26652.58920659]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 16716.08751987]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 10609.73660535]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 13284.12563472]]),\n",
       "  array([[ 3056.05354911]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[ 6920.65390442]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]]),\n",
       "  array([[-0.]])])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trains_x, trains_y, alpha=0.01, epochs=100, tests_x=tests_x, tests_y=tests_y, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19.20443274])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([2,3,4])\n",
    "np.dot(a, b) + np.random.randn(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per(x_data, y_data, weight):\n",
    "    y_pre = np.where(np.dot(x_data, weight)>0, 1, -1)\n",
    "    return np.sum(y_pre == y_data)*1. / y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_train(x_train, y_train, epochs=100, is_pocket=False, init_weight_radio=0.1, alpha=1):\n",
    "    M , N = x_train.shape\n",
    "    W = init_weight_radio * np.random.randn(N, )\n",
    "#     b = init_weight_radio * np.random.randn(1, )\n",
    "    count = 0\n",
    "    while True:\n",
    "        for i in range(M):\n",
    "            if accuracy_per(x_train, y_train, W) == 1:\n",
    "                return (W, b, count)\n",
    "            while True:\n",
    "                y_pre = np.where(np.dot(x_train[i], W) > 0, 1, -1)\n",
    "                if y_pre == y_train[i]:\n",
    "                    break\n",
    "                else:\n",
    "                    W += alpha * y_train[i] * x_train[i]\n",
    "                    # b += alpha * y_train[i]\n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path='../data/coursera_mlf.csv'):\n",
    "    data = pd.read_csv(filepath_or_buffer=path, delim_whitespace=True, header=None)\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:, :-1]\n",
    "y_train = data[:, -1].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e06fd7341140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_weight_radio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-6870399dd46d>\u001b[0m in \u001b[0;36mperceptron_train\u001b[0;34m(x_train, y_train, epochs, is_pocket, init_weight_radio, alpha)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0maccuracy_per\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-f03b1efa85f9>\u001b[0m in \u001b[0;36maccuracy_per\u001b[0;34m(x_data, y_data, weight)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy_per\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pre\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W, b, count = perceptron_train(x_train, y_train, init_weight_radio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
